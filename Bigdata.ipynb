{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data project \n",
    "### Lo Yi 22019023D\n",
    "### Li Ho Ming Homan 22017386D\n",
    "### Liu Kwan Lok 22029662D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "Both basic and advanced data exploration are conducted in the DEA. Basic exploration focuses on the properties of a single component of the dataset, while advanced exploration illustrates the structure of the dataset.\n",
    "\n",
    "### Basic Data Exploration\n",
    "\n",
    "Unique values, null objects, duplicated data, sentiment annotations, and review samples with positive and negative sentiment are scanned in this section. This provides the direction to support further data exploration and pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ho\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\ho\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\ho\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\ho\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\ho\\anaconda3\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ho\\anaconda3\\lib\\site-packages (from xgboost) (1.25.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\ho\\anaconda3\\lib\\site-packages (from xgboost) (1.10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ho\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ho\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ho\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ho\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ho\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ho\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\ho\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from gensim) (1.25.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from gensim) (1.10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ho\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ho\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ho\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ho\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ho\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ho\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ho\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Collecting en-core-web-lg==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from en-core-web-lg==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (21.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.15)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.25.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ho\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ho\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ho\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (61.2.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ho\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ho\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ho\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.16)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ho\\appdata\\roaming\\python\\python39\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ho\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.1)\n",
      "âœ” Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ho\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\ho\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\ho\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\ho\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "2024-04-30 16:45:48.897593: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING:tensorflow:From c:\\Users\\ho\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ho\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ho\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ho\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ho\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ho\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ho\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "!pip install xgboost\n",
    "!pip install gensim\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "movReview=pd.read_csv('./IMDB Dataset.csv')\n",
    "print(movReview.shape)\n",
    "movReview.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all sentiment to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1\n",
       "5  Probably my all-time favorite movie, a story o...          1\n",
       "6  I sure would like to see a resurrection of a u...          1\n",
       "7  This show was an amazing, fresh & innovative i...          0\n",
       "8  Encouraged by the positive comments about this...          0\n",
       "9  If you like original gut wrenching laughter yo...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movReview = movReview.replace(to_replace = \"positive\", value = 1)\n",
    "movReview = movReview.replace(to_replace = \"negative\", value = 0)\n",
    "movReview.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No null values are detected in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movReview.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is found that 96 reviews are the same. These duplicated data will not cause data imbalance in this project as it is an insignificant portion of the dataset (418*2/50000 = 1.67% < 2%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(movReview[movReview.duplicated()].head())\n",
    "print(\"Duplicated reviews:\", movReview[movReview.duplicated()].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both positive and negative reviews are equally distributed across the dataset. Therefore, up-sampling and down-weighting approaches are not required to be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "movReview.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the reviews below, mistakes such as inconsistent sentence structure, bad syntax, colloquialism, html tags, wrong spellings, abbreviations, and lousy casing, are being identified. Moreover, it is observed that a single review contains a mixture of positive and negative wordings. In addition, sentiment-unrelated paragraphs are noticed within these comments. This may impact the sensitivity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "movReview[movReview[\"sentiment\"] == 0][\"review\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one reivew from positive sentiment column and negative sentiment column.\n",
    "pos=movReview[movReview[\"sentiment\"] == 1][\"review\"][13466]\n",
    "neg=movReview[movReview[\"sentiment\"] == 0][\"review\"][4103]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive Sentiment:\\n\\n\"+pos+\"\\n\\n\"+\"Negative Sentiment:\\n\\n\"+neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Data Exploration\n",
    "\n",
    "The structure of both negative and positive reviews is contrasted to determine whether the dataset is formed with equal properties. A classification model with a balanced dataset would generate more accurate and balanced models.\n",
    "\n",
    "Before pre-processing, we assumed that audiences with different sentiments used distinctive vocabs to express their feelings. Thus, the vocabs in reviews are being evaluated to see if it is a suitable feature to differentiate between positive sentiment and negative sentiment.\n",
    "\n",
    "Libraries that check for stop words, tokenize text, and identify collocation are imported to conduct in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from IPython.display import display_html\n",
    "from itertools import chain,cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the average word length\n",
    "def avg_word(sentence):\n",
    "    return(sum(len(word) for word in sentence.split())/len(sentence))\n",
    "\n",
    "# Counting the words\n",
    "def word_count(sentence):\n",
    "    return(len(sentence.split(\" \")))\n",
    "\n",
    "# Counting the number of characters\n",
    "def char_count(sentence):\n",
    "    return(len(sentence))\n",
    "\n",
    "# Identifying the number of stop words in each review\n",
    "def stop_words(sentence):\n",
    "    return(len([x for x in word_tokenize(sentence) if x in stop]))\n",
    "\n",
    "# Locating special characters in each review\n",
    "def special_chars(str):\n",
    "    special = 0\n",
    "    for i in range(len(str)):\n",
    "        if str[i].isupper() | str[i].islower()|str[i].isdigit()|str[i].isspace():\n",
    "            continue\n",
    "        else:\n",
    "            special+= 1\n",
    "    return(special)\n",
    "\n",
    "# Return a dataframe that conclude the overall statistics of basic features\n",
    "def basic_features_extraction(text):\n",
    "     return(pd.DataFrame({\"avg_word\":text.apply(lambda x: avg_word(x)),\n",
    "                         \"word_count\":text.apply(lambda x: word_count(x)),\n",
    "                         \"char_count\":text.apply(lambda x: char_count(x)),\n",
    "                         \"stop_words\":text.apply(lambda x: stop_words(x)),\n",
    "                         \"special_chars\":text.apply(lambda x: special_chars(x))}).describe())\n",
    "    \n",
    "# Displaying the results side by side for easier comparison\n",
    "def display_side_by_side(*args,titles=cycle([''])):\n",
    "    html_str=''\n",
    "    for df,title in zip(args, chain(titles,cycle(['</br>'])) ):\n",
    "        html_str+='<th style=\"text-align:center\"><td style=\"vertical-align:top\">'\n",
    "        html_str+=f'<h2>{title}</h2>'\n",
    "        html_str+=df.to_html().replace('table','table style=\"display:inline\"')\n",
    "        html_str+='</td></th>'\n",
    "    display_html(html_str,raw=True)\n",
    "\n",
    "# Group by the number of words for each review \n",
    "def word_count_extraction(text):\n",
    "    return(pd.DataFrame({\"word_count\":text.apply(lambda x: word_count(x))}).groupby(['word_count']).\\\n",
    "    size().sort_values(ascending=False).reset_index(name ='total_word_count'))\n",
    "\n",
    "# Viewing the distribution of word counts across the dataset\n",
    "def histogram(df,x,y,selection=None,fun=None):\n",
    "    if selection is not None:\n",
    "        if fun=='n':\n",
    "            fig = px.histogram(df, x=x, y=y,color=selection)\n",
    "        else :\n",
    "            fig = px.histogram(df, x=x, y=y,color=selection, histfunc=fun, barmode=\"group\")\n",
    "        return (fig.show())\n",
    "    else:\n",
    "        fig = px.histogram(df, x=x, y=y)\n",
    "        fig.update_xaxes(range=[0, 1550])\n",
    "        fig.update_yaxes(range=[0, 3600])\n",
    "        return (fig.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence structure between the reviews with positive sentiment and reviews with negative sentiment is similar. It is concluded that the dataset is balanced with similar properties and the same pre-processing method can be applied to both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2>Positive Sentiment</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_word</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>special_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000.000000</td>\n",
       "      <td>25000.000000</td>\n",
       "      <td>25000.000000</td>\n",
       "      <td>25000.000000</td>\n",
       "      <td>25000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.823537</td>\n",
       "      <td>232.837760</td>\n",
       "      <td>1324.797680</td>\n",
       "      <td>99.397320</td>\n",
       "      <td>51.661640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.011074</td>\n",
       "      <td>177.475492</td>\n",
       "      <td>1031.492627</td>\n",
       "      <td>77.161442</td>\n",
       "      <td>44.110415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.554381</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.816839</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>691.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.823797</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>968.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.830579</td>\n",
       "      <td>284.000000</td>\n",
       "      <td>1614.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.926027</td>\n",
       "      <td>2470.000000</td>\n",
       "      <td>13704.000000</td>\n",
       "      <td>1059.000000</td>\n",
       "      <td>657.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th><th style=\"text-align:center\"><td style=\"vertical-align:top\"><h2>Negative Sentiment</h2><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_word</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>special_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000.000000</td>\n",
       "      <td>25000.00000</td>\n",
       "      <td>25000.000000</td>\n",
       "      <td>25000.000000</td>\n",
       "      <td>25000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.822530</td>\n",
       "      <td>229.45412</td>\n",
       "      <td>1294.064360</td>\n",
       "      <td>98.332840</td>\n",
       "      <td>53.687960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.010126</td>\n",
       "      <td>164.93448</td>\n",
       "      <td>945.892669</td>\n",
       "      <td>71.928071</td>\n",
       "      <td>43.955937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.777027</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.816090</td>\n",
       "      <td>128.00000</td>\n",
       "      <td>706.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.822474</td>\n",
       "      <td>174.00000</td>\n",
       "      <td>973.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.828997</td>\n",
       "      <td>278.00000</td>\n",
       "      <td>1567.250000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>68.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.941176</td>\n",
       "      <td>1522.00000</td>\n",
       "      <td>8969.000000</td>\n",
       "      <td>634.000000</td>\n",
       "      <td>625.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"></td></th>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_basic_features=basic_features_extraction(movReview[movReview[\"sentiment\"] == 1].review)\n",
    "neg_basic_features=basic_features_extraction(movReview[movReview[\"sentiment\"] == 0].review)\n",
    "display_side_by_side(pos_basic_features, neg_basic_features, titles=['Positive Sentiment', 'Negative Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the below histograms, both graphs have the same right-skewed shape, which represents that both reviews have the same distribution pattern of words. This justifies that the dataset is balanced with equal number of input samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(word_count_extraction(movReview[movReview[\"sentiment\"] == 1].review), 'word_count', 'total_word_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(word_count_extraction(movReview[movReview[\"sentiment\"] == 0].review), 'word_count', 'total_word_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the occurrence of vocabs in each review\n",
    "def vocab_count(dataframe, text, selection):\n",
    "    return(pd.Series(' '.join(dataframe[dataframe[\"sentiment\"]==selection][text]).split()).value_counts())\n",
    "\n",
    "# Showing the vocabs list that used most often in the specific sentiment\n",
    "def vocab_list(dataframe, text, selection):\n",
    "    \n",
    "    df=pd.concat([vocab_count(dataframe, text, 0),vocab_count(dataframe, text, 1)],axis=1)\n",
    "    df=df.fillna(0)\n",
    "    df.columns = ['neg', 'pos']\n",
    "\n",
    "    if selection=='pos':\n",
    "        return(pd.DataFrame({selection:df[df[selection] > df['neg']][selection]}).sort_values(by=selection, ascending=False))\n",
    "    if selection=='neg':\n",
    "        return(pd.DataFrame({selection:df[df[selection] > df['pos']][selection]}).sort_values(by=selection, ascending=False))\n",
    "               \n",
    "# Display the comparison in the form of horizontal barchart\n",
    "def hori_barchart(df,x,y,selection,title):\n",
    "    fig = px.histogram(df, x=y, y=x, height=700,color=selection,barmode=\"group\")\n",
    "    fig.update_layout(yaxis={'categoryorder':'total ascending'})\n",
    "    return (fig.show())\n",
    "    \n",
    "def change_id(df):\n",
    "    df=pd.DataFrame(df,columns=['value'])\n",
    "    df['id']=df.index\n",
    "    return(df)\n",
    "\n",
    "# Comparing the number of particular words in both sentiments \n",
    "def word_comparison(text):\n",
    "    df_neg=change_id(vocab_count(movReview, text, 0))\n",
    "    df_pos=change_id(vocab_count(movReview, text, 1))\n",
    "\n",
    "    df_neg['sentiment'], df_pos['sentiment']=0, 1\n",
    "\n",
    "    df_combine=pd.concat([df_pos[:20], df_neg[:20]],ignore_index=True).sort_values(by ='value', ascending = False)\n",
    "\n",
    "    hori_barchart(df_combine, 'id', 'value', 'sentiment', 'Word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below charts suggested that both sentiments own different types of wordings. However, the statement of using distinctive words to identify sentiments is yet to be verified at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_side_by_side(vocab_list(movReview, 'review', \"pos\")[:49], vocab_list(movReview, 'review', \"neg\")[:49], titles=['Positive Sentiment', 'Negative Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words in both sentiments\n",
    "word_comparison('review')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "Performing text pre-processing requires libraries that return the wordnet object value corresponding to the POS tag, allow text filtering and manipulation, and predict sentiment predicting. During this process, stop words, special characters, non-alphabetic letters, html-tags, and punctuations are removed. In addition, the words that match with a specific part of speech, such as nouns, verbs, adjectives, and adverbs are being transformed and lemmatized.\n",
    "\n",
    "\n",
    "Although misspelling is observed in the data exploration, the method of correcting the spelling (TextBlob) consumes an enormous amount of computing resources. This may be considered in future projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid_obj = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting english words with the correct format.\n",
    "def filter_stop_br_special(text):\n",
    "    texts=[]\n",
    "    for i in word_tokenize(text) :\n",
    "            temp_word=''\n",
    "            for x in range(len(i)):\n",
    "                if i[x].isupper() | i[x].islower():\n",
    "                    temp_word+=i[x].lower()\n",
    "                else:\n",
    "                    break\n",
    "            if len(temp_word)>1 and len(temp_word) == len(i) and temp_word not in stop and temp_word != 'br' :\n",
    "                texts.append(temp_word)\n",
    "    return(' '.join(texts))\n",
    "\n",
    "# Returning the wordnet object that corresponding to part of speech tag\n",
    "def get_wordnet_pos(pos_tags):\n",
    "    pos=pos_tags[0]\n",
    "    if pos=='J':\n",
    "        return wordnet.ADJ\n",
    "    elif pos=='V':\n",
    "        return wordnet.VERB\n",
    "    elif pos=='N':\n",
    "        return wordnet.NOUN\n",
    "    elif pos=='R':\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# lemmatizing words that match with specific part of speech tag\n",
    "def process(text):\n",
    "    pos_tags = pos_tag(word_tokenize(filter_stop_br_special(text)))\n",
    "    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "    return(' '.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process function is enforced on the positive and negative samples. The overall neutral score is dropped slightly, while the negative or positive scores are enhanced in both cases. In addition, the compound score, which suggests the intensity of certain sentiments, has been rectified. This supports that the processing has greatly improved the sensitivity of the sentiment analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples of reviews before and after processing\n",
    "print(\"Positive Review (Before processing):\\n\\n\"+pos+\"\\n\\n\"+\n",
    "      str(sid_obj.polarity_scores(pos))+\"\\n\\n\"+\n",
    "     \"Positive Review (After processing):\\n\\n\"+process(pos)+\"\\n\\n\"+\n",
    "      str(sid_obj.polarity_scores(process(pos)))+\"\\n\\n\")\n",
    "\n",
    "print(\"Negative Review (Before processing):\\n\\n\"+neg+\"\\n\\n\"+\n",
    "      str(sid_obj.polarity_scores(neg))+\"\\n\\n\"+\n",
    "     \"Negative Review (After processing):\\n\\n\"+process(neg)+\"\\n\\n\"+\n",
    "      str(sid_obj.polarity_scores(process(neg))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>one reviewer mention watch oz episode hook rig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "      <td>wonderful little production film technique fas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>think wonderful way spend time hot summer week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>1</td>\n",
       "      <td>probably favorite movie story selflessness sac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>1</td>\n",
       "      <td>sure would like see resurrection date seahunt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>0</td>\n",
       "      <td>show amazing fresh innovative idea first air f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>0</td>\n",
       "      <td>encourage positive comment film look forward w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>1</td>\n",
       "      <td>like original gut wrench laughter like movie y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...          1   \n",
       "1  A wonderful little production. <br /><br />The...          1   \n",
       "2  I thought this was a wonderful way to spend ti...          1   \n",
       "3  Basically there's a family where a little boy ...          0   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1   \n",
       "5  Probably my all-time favorite movie, a story o...          1   \n",
       "6  I sure would like to see a resurrection of a u...          1   \n",
       "7  This show was an amazing, fresh & innovative i...          0   \n",
       "8  Encouraged by the positive comments about this...          0   \n",
       "9  If you like original gut wrenching laughter yo...          1   \n",
       "\n",
       "                                        review_clean  \n",
       "0  one reviewer mention watch oz episode hook rig...  \n",
       "1  wonderful little production film technique fas...  \n",
       "2  think wonderful way spend time hot summer week...  \n",
       "3  basically family little boy jake think zombie ...  \n",
       "4  petter mattei love time money visually stunnin...  \n",
       "5  probably favorite movie story selflessness sac...  \n",
       "6  sure would like see resurrection date seahunt ...  \n",
       "7  show amazing fresh innovative idea first air f...  \n",
       "8  encourage positive comment film look forward w...  \n",
       "9  like original gut wrench laughter like movie y...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying the preprocessing function on the dataset\n",
    "movReview[\"review_clean\"]=movReview[\"review\"].apply(lambda x: process(x))\n",
    "movReview.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of words has been halved after processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_basic_features=basic_features_extraction(movReview[movReview[\"sentiment\"] == 1].review_clean)\n",
    "neg_basic_features=basic_features_extraction(movReview[movReview[\"sentiment\"] == 0].review_clean)\n",
    "display_side_by_side(pos_basic_features, neg_basic_features, titles=['Positive Sentiment', 'Negative Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the below figures, the pre-processing function does not affect the balance of the dataset. In addition, the right skewness of the histogram has been increased significantly due to drastic decrease in words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(word_count_extraction(movReview[movReview[\"sentiment\"] == 1].review_clean), 'word_count', 'total_word_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(word_count_extraction(movReview[movReview[\"sentiment\"] == 0].review_clean), 'word_count', 'total_word_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing, the contrast between negative and positive reviews becomes significant. Reviews with positive sentiment use the word \"film\". On the contrary, reviews with negative sentiments used the word \"movie\" frequently.\n",
    "\n",
    "It is noticed that both reviews are mainly concerned about the story and cast. This provides perspective on how the audience evaluates the movie or film. Reviewers with a positive sentiment like to use the word \"story\" and \"role\" to deliver the same meaning. On the other hand, reviewers with negative emotions criticize the story and cast with the words \"plot\" and \"actor\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_side_by_side(vocab_list(movReview, 'review_clean', \"pos\")[:49], vocab_list(movReview, 'review_clean', \"neg\")[:49], titles=['Positive Sentiment', 'Negative Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words in both sentiments\n",
    "word_comparison('review_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Analytics\n",
    "\n",
    "Only knowing which single word contributes the most portion of the dataset limited the scope of knowledge discovery. The collocations withdrawn from these reviews offer important aspects of understanding the context of the reviews. In addition, the collocated pairs reveal the sequence and patterns of words and enhance comprehension of the association of words. This permits us to predict what kinds of words may be found together in different scenarios.\n",
    "\n",
    "The paradigmatic association of these collocated pairs is also studied as the audience might use similar words interchangeably to express the same meaning or context. This enables us to discuss popular taste with a wide spectrum of perspectives, yield valuable insights that reveal behaviors or phenomena, and suggest solutions or ideas that are intrinsic to the business' success.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding collocated paries\n",
    "def bi_gram(dataframe, series):\n",
    "    occurrences = {}\n",
    "    bi_gram=[]\n",
    "\n",
    "    dataframe['bi_gram']=series.apply(lambda x: BigramCollocationFinder.from_words(word_tokenize(x)).\\\n",
    "                                      nbest(BigramAssocMeasures.likelihood_ratio, 100))\n",
    "    for i in dataframe['bi_gram']:\n",
    "        bi_gram.extend(i)\n",
    "    \n",
    "    for i in bi_gram:\n",
    "        if i in occurrences:\n",
    "            occurrences[i] += 1\n",
    "        else:\n",
    "            occurrences[i] = 1\n",
    "    return(sorted(occurrences.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Generating the similar words of the collocated pairs\n",
    "def similarity(collocated_pairs, words, degree):\n",
    "    \n",
    "    simi_list=[]\n",
    "    word_choice=nlp(words) \n",
    "    \n",
    "    for i in collocated_pairs:\n",
    "        if word_choice.similarity(nlp(' '.join(i[0])))>degree:\n",
    "            simi_list.append(i)\n",
    "    return(simi_list)\n",
    "\n",
    "# Comparison between the properties of positive and negative reviews\n",
    "def neg_pos_df(negative, positive):\n",
    "    \n",
    "    negative_length=len(negative)\n",
    "    positive_length=len(positive)\n",
    "    \n",
    "    if negative_length < positive_length:\n",
    "        return(pd.DataFrame({\"Negative Reviews\":negative[:negative_length], \"Positive Reviews\":positive[:negative_length]}))\n",
    "    elif positive_length < negative_length:\n",
    "        return(pd.DataFrame({\"Negative Reviews\":negative[:positive_length], \"Positive Reviews\":positive[:positive_length]}))\n",
    "    else:\n",
    "        return(pd.DataFrame({\"Negative Reviews\":negative[:negative_length], \"Positive Reviews\":positive[:negative_length]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following list of collocated pairs suggests the aspects that we should look into to further understand the underlying meaning of both emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_bi_gram=bi_gram(movReview[movReview[\"sentiment\"]==0], movReview[movReview[\"sentiment\"]==0][\"review_clean\"])\n",
    "pos_bi_gram=bi_gram(movReview[movReview[\"sentiment\"]==1], movReview[movReview[\"sentiment\"]==1][\"review_clean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_df([i for i in neg_bi_gram if i[1] > 100], [i for i in pos_bi_gram if i[1] > 100]).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First 10000 rows are extracted to save computing resources\n",
    "neg_bi=neg_bi_gram[:10000]\n",
    "pos_bi=pos_bi_gram[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polar Emotions are suggested from the comparison. Words, such as \"great\", \"excellent\", and \"amazing\", are used to compliment the film, while feeling words, such as \"bad\", \"awful\", and \"terrible\", occur frequently in the negative reviews. Moreover, the phenomenon of split views is observed in the negative segment.\n",
    "\n",
    "We see that people generally have negative sentiment towards movies in this reviews dataset. There are only a few movies that are being commented as great. It is worth mentioning that not all negative reviews consider movies as bad, but rather have certain aspects in the movie that they dislike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_df(similarity(neg_bi, 'bad movie', 0.93), similarity(pos_bi, 'great movie', 0.93))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both sides consider story as a key element when commenting on the movie. People with positive reviews towards a movie are more sensitive towards the story. Overall, both positive and negative reviews use a wide range of adjectives to describe the plot, rather than just \"good\" or \"bad\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_df(similarity(neg_bi, 'bad story', 0.9), similarity(pos_bi, 'great story', 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both positive and negative reviewers also care a lot about acting. Negative reviews are more sensitive towards bad acting. However, it is not elaborated what kind of acting constitutes as bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_df(similarity(neg_bi, 'bad act', 0.9), similarity(pos_bi, 'great act', 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drill-in Analytics\n",
    "\n",
    "From the perspective of movies' properties, it is seen in the data that most of reviewers watch movies that have special effects, which is relevant in recent times, as sci-fi or superhero genre films nowadays contain a lot of special effects to attract their audiences. It is also seen that people are slightly negative towards these movies. Movies with low budget also usually attract more negative reviews. On the other hand, high-budgeted movies attract more positive reviews. \n",
    "\n",
    "People also do not like sex or nude scenes, and view these movies more negatively. Similarly, people also usually give negative reviews if the movie contains fight scenes, however the skew towards negative reviews is less than that of sex scenes. However, battle scenes are viewed more favorably in a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_df(similarity(neg_bi, 'special effect', 0.85), similarity(pos_bi, 'special effect', 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_df(similarity(neg_bi, 'low budget', 0.85), similarity(pos_bi, 'low budget', 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_df(similarity(neg_bi, 'sex scene', 0.85), similarity(pos_bi, 'sex scene', 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_df(similarity(neg_bi, 'fight scene', 0.85), similarity(pos_bi, 'fight scene', 0.85))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of genre, horror movies or horror flicks attract the most negative reviews, while romantic comedies, dramas, and comedies having the most positive reviews. In addition, movies based on true stories are preferred by the audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_df(similarity(neg_bi, 'horror movie', 0.9), similarity(pos_bi, 'horror movie', 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_df(similarity(neg_bi, 'romantic comedy', 0.85), similarity(pos_bi, 'romantic comedy', 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_df(similarity(neg_bi, 'true story', 0.9), similarity(pos_bi, 'true story', 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that acting from the main character affects people's views and sentiment towards the movie more than that of the support cast. People with negative reviews towards a movie are more sensitive towards the main character's performance than people with positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_df(similarity(neg_bi, 'main character', 0.9), similarity(pos_bi, 'main character', 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos_df(similarity(neg_bi, 'support cast', 0.85), similarity(pos_bi, 'support cast', 0.85))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder as le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying features, such as polarity scores, numeric vectors, TF-IDF, words count, and character counts on the dataset.\n",
    "def feature_engine(dataframe, series):\n",
    "    temp=list(dataframe['sentiment'].values)\n",
    "    \n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    polar=series.apply(lambda x: sid_obj.polarity_scores(x)).apply(pd.Series)\n",
    "    \n",
    "    Doc2VecModel = Doc2Vec([TaggedDocument(word_tokenize(doc), [i]) for i, doc in series.iteritems()],\\\n",
    "                           vector_size=10, window=2, min_count=1, workers=5)\n",
    "\n",
    "    doc2vec=series.apply(lambda x: Doc2VecModel.infer_vector(word_tokenize(x))).apply(pd.Series)\n",
    "\n",
    "    tfidf = TfidfVectorizer(lowercase=True, analyzer='word',stop_words= 'english', ngram_range=(1,1), min_df = 5)\n",
    "\n",
    "    TFIDF=pd.DataFrame(tfidf.fit_transform(series).toarray(),columns = [\"tfidf_\"+i for i in tfidf.get_feature_names_out()])\n",
    "    \n",
    "    word_count_metric=pd.DataFrame({'word_count_metric':series.apply(lambda x: word_count(x))})\n",
    "                                   \n",
    "    character_count_metric=pd.DataFrame({'character_count_metric':series.apply(lambda x: char_count(x))})\n",
    "    \n",
    "    dataframe=pd.concat([polar.reset_index(drop=True),\\\n",
    "                         doc2vec.reset_index(drop=True),\\\n",
    "                         TFIDF.reset_index(drop=True),\\\n",
    "                        word_count_metric.reset_index(drop=True),\n",
    "                        character_count_metric.reset_index(drop=True)], axis=1).apply(lambda x:le().fit_transform(x))\n",
    "\n",
    "    dataframe['sentiment']=temp\n",
    "    \n",
    "    return(dataframe)\n",
    "\n",
    "def getTFIDF(series):\n",
    "    tfidf = TfidfVectorizer(lowercase=True, analyzer='word', stop_words='english', ngram_range=(1,1), min_df=5)\n",
    "    tfidf_matrix = tfidf.fit_transform(series)\n",
    "    return tfidf, tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Step 1: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(movReview[\"review_clean\"].values, movReview[\"sentiment\"].values, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 2: Fit the LDA model on the training data\n",
    "tfidf_vectorizer, tfidf_matrix_train = getTFIDF(X_train)\n",
    "lda = LatentDirichletAllocation(n_components=200, random_state=123, learning_method='batch')\n",
    "lda.fit(tfidf_matrix_train)\n",
    "\n",
    "# Step 3: Transform both training and testing data into topic distributions\n",
    "X_train = lda.transform(tfidf_matrix_train)\n",
    "X_test = lda.transform(tfidf_vectorizer.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout , Activation\n",
    "from keras.layers import LSTM , Embedding\n",
    "from keras.layers import MaxPooling1D , GlobalMaxPooling1D,Conv1D , Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 7000\n",
    "max_words = 200\n",
    "X_train_cnn = sequence.pad_sequences(X_train , maxlen = max_words)\n",
    "X_test_cnn = sequence.pad_sequences(X_test , maxlen = max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(top_words , 32 , input_length = max_words))\n",
    "\n",
    "model.add(Conv1D(128 , 5 , padding = 'same',activation = 'relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "\n",
    "model.add(Conv1D(128 , 5 , padding = 'same',activation = 'relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "\n",
    "model.add(Conv1D(128 , 5 , padding = 'same',activation = 'relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(120, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 200, 32)           224000    \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, 200, 128)          20608     \n",
      "                                                                 \n",
      " max_pooling1d_9 (MaxPoolin  (None, 40, 128)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          (None, 40, 128)           82048     \n",
      "                                                                 \n",
      " max_pooling1d_10 (MaxPooli  (None, 8, 128)            0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, 8, 128)            82048     \n",
      "                                                                 \n",
      " max_pooling1d_11 (MaxPooli  (None, 1, 128)            0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 250)               32250     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 120)               30120     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 121       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 471195 (1.80 MB)\n",
      "Trainable params: 471195 (1.80 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = 'accuracy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 - 3s - loss: 0.6932 - accuracy: 0.4961 - val_loss: 0.6931 - val_accuracy: 0.4993 - 3s/epoch - 15ms/step\n",
      "Epoch 2/2\n",
      "196/196 - 3s - loss: 0.6932 - accuracy: 0.4964 - val_loss: 0.6932 - val_accuracy: 0.4993 - 3s/epoch - 14ms/step\n",
      "782/782 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.50      1.00      0.67     12483\n",
      "    Positive       0.00      0.00      0.00     12517\n",
      "\n",
      "    accuracy                           0.50     25000\n",
      "   macro avg       0.25      0.50      0.33     25000\n",
      "weighted avg       0.25      0.50      0.33     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_cnn, y_train, validation_data=(X_test_cnn, y_test), batch_size=128, epochs=2, verbose=2)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_prob = model.predict(X_test_cnn)\n",
    "y_pred = (y_pred_prob > 0.5).astype('int')  # Threshold probabilities to get binary predictions\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Positive'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT \n",
    "### Advance algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "# Convert TF-IDF and LDA data to PyTorch tensors\n",
    "tfidf_values_train = torch.tensor(X_train, dtype=torch.long)\n",
    "tfidf_values_test = torch.tensor(X_test, dtype=torch.long)\n",
    "# Combine input features\n",
    "train_input_ids = tfidf_values_train\n",
    "test_input_ids = tfidf_values_test\n",
    "\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_dataset = TensorDataset(train_input_ids, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        input_ids, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_accuracy = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, labels = batch\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        test_loss += loss.item()\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        test_accuracy += (preds == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy /= len(test_loader.dataset)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Positive'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contain all models that are needed to train\n",
    "def modelComp(X_train_fun, X_test_fun, y_train_fun, y_test_func):\n",
    "    models={'Random Forest':RandomForestClassifier(max_features=200, n_estimators=300, random_state=42),\n",
    "            'XGBoost':XGBClassifier()}\n",
    "    \n",
    "    classifreport={}\n",
    "    for key in models.keys():\n",
    "        \n",
    "        models[key].fit(X_train_fun, y_train_fun)\n",
    "        \n",
    "        predictions = models[key].predict(X_test_fun)\n",
    "        classifreport[key]=classification_report(y_test_func, predictions, target_names=['0','1'])\n",
    "\n",
    "\n",
    "    for k, v in classifreport.items():\n",
    "        print(\"\\n\\n\"+k)\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Models (Random forest, XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.61      0.59     12483\n",
      "           1       0.59      0.56      0.58     12517\n",
      "\n",
      "    accuracy                           0.59     25000\n",
      "   macro avg       0.59      0.59      0.59     25000\n",
      "weighted avg       0.59      0.59      0.59     25000\n",
      "\n",
      "\n",
      "\n",
      "XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.62      0.60     12483\n",
      "           1       0.60      0.56      0.58     12517\n",
      "\n",
      "    accuracy                           0.59     25000\n",
      "   macro avg       0.59      0.59      0.59     25000\n",
      "weighted avg       0.59      0.59      0.59     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train.columns = X_train.columns.astype(str)\n",
    "X_test.columns = X_test.columns.astype(str)\n",
    "modelComp(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters tunning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "[CV] END criterion=gini, max_depth=4, max_features=auto, n_estimators=200; total time=   6.6s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=auto, n_estimators=200; total time=   6.5s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=auto, n_estimators=200; total time=   6.5s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=auto, n_estimators=200; total time=   6.5s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=auto, n_estimators=200; total time=   6.5s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=auto, n_estimators=500; total time=  16.5s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=auto, n_estimators=500; total time=  16.2s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=auto, n_estimators=500; total time=  16.3s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=auto, n_estimators=500; total time=  16.3s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=auto, n_estimators=500; total time=  16.2s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=200; total time=   6.5s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=200; total time=   6.4s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=200; total time=   6.4s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=200; total time=   6.3s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=200; total time=   6.4s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=500; total time=  16.1s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=500; total time=  15.9s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=500; total time=  16.2s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=500; total time=  16.5s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=500; total time=  16.3s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=log2, n_estimators=200; total time=   3.5s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=log2, n_estimators=200; total time=   3.5s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=log2, n_estimators=200; total time=   3.6s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=log2, n_estimators=200; total time=   3.5s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=log2, n_estimators=200; total time=   3.5s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=log2, n_estimators=500; total time=   8.8s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=log2, n_estimators=500; total time=   8.7s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=log2, n_estimators=500; total time=   8.8s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=log2, n_estimators=500; total time=   8.8s\n",
      "[CV] END criterion=gini, max_depth=4, max_features=log2, n_estimators=500; total time=   8.7s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=auto, n_estimators=200; total time=   8.0s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=auto, n_estimators=200; total time=   7.9s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=auto, n_estimators=200; total time=   7.9s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=auto, n_estimators=200; total time=   7.9s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=auto, n_estimators=200; total time=   8.0s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=auto, n_estimators=500; total time=  20.0s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=auto, n_estimators=500; total time=  20.0s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=auto, n_estimators=500; total time=  19.9s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=auto, n_estimators=500; total time=  19.9s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=auto, n_estimators=500; total time=  20.0s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=200; total time=   8.1s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=200; total time=   7.9s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=200; total time=   7.9s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=200; total time=   8.0s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=200; total time=   7.9s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=500; total time=  20.1s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=500; total time=  19.9s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=500; total time=  20.0s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=500; total time=  20.0s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=500; total time=  20.0s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=log2, n_estimators=200; total time=   4.2s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=log2, n_estimators=200; total time=   4.2s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=log2, n_estimators=200; total time=   4.2s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=log2, n_estimators=200; total time=   4.2s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=log2, n_estimators=200; total time=   4.2s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=log2, n_estimators=500; total time=  10.6s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=log2, n_estimators=500; total time=  10.7s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=log2, n_estimators=500; total time=  10.5s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=log2, n_estimators=500; total time=  10.7s\n",
      "[CV] END criterion=gini, max_depth=5, max_features=log2, n_estimators=500; total time=  10.6s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=auto, n_estimators=200; total time=   9.4s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=auto, n_estimators=200; total time=   9.5s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=auto, n_estimators=200; total time=   9.4s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=auto, n_estimators=200; total time=   9.4s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=auto, n_estimators=200; total time=   9.6s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=auto, n_estimators=500; total time=  23.8s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=auto, n_estimators=500; total time=  23.7s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=auto, n_estimators=500; total time=  23.7s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=auto, n_estimators=500; total time=  23.7s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=auto, n_estimators=500; total time=  23.6s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=sqrt, n_estimators=200; total time=   9.4s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=sqrt, n_estimators=200; total time=   9.7s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=sqrt, n_estimators=200; total time=  10.0s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=sqrt, n_estimators=200; total time=   9.6s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=sqrt, n_estimators=200; total time=   9.4s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=sqrt, n_estimators=500; total time=  23.8s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=sqrt, n_estimators=500; total time=  24.4s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=sqrt, n_estimators=500; total time=  24.0s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=sqrt, n_estimators=500; total time=  23.7s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=sqrt, n_estimators=500; total time=  23.7s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=log2, n_estimators=200; total time=   4.9s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=log2, n_estimators=200; total time=   5.0s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=log2, n_estimators=200; total time=   4.9s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=log2, n_estimators=200; total time=   4.9s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=log2, n_estimators=200; total time=   4.9s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=log2, n_estimators=500; total time=  12.5s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=log2, n_estimators=500; total time=  12.5s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=log2, n_estimators=500; total time=  12.5s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=log2, n_estimators=500; total time=  12.5s\n",
      "[CV] END criterion=gini, max_depth=6, max_features=log2, n_estimators=500; total time=  12.5s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=auto, n_estimators=200; total time=  11.1s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=auto, n_estimators=200; total time=  10.9s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=auto, n_estimators=200; total time=  11.0s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=auto, n_estimators=200; total time=  10.9s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=auto, n_estimators=200; total time=  10.9s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=auto, n_estimators=500; total time=  27.4s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=auto, n_estimators=500; total time=  27.4s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=auto, n_estimators=500; total time=  27.5s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=auto, n_estimators=500; total time=  27.3s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=auto, n_estimators=500; total time=  26.8s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=sqrt, n_estimators=200; total time=  10.4s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=sqrt, n_estimators=200; total time=  10.6s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=sqrt, n_estimators=200; total time=  10.9s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=sqrt, n_estimators=200; total time=  10.9s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=sqrt, n_estimators=200; total time=  10.9s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=sqrt, n_estimators=500; total time=  27.2s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=sqrt, n_estimators=500; total time=  27.3s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=sqrt, n_estimators=500; total time=  27.4s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=sqrt, n_estimators=500; total time=  27.3s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=sqrt, n_estimators=500; total time=  27.5s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=log2, n_estimators=200; total time=   5.7s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=log2, n_estimators=200; total time=   5.7s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=log2, n_estimators=200; total time=   5.8s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=log2, n_estimators=200; total time=   5.7s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=log2, n_estimators=200; total time=   5.7s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=log2, n_estimators=500; total time=  14.4s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=log2, n_estimators=500; total time=  14.3s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=log2, n_estimators=500; total time=  14.4s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=log2, n_estimators=500; total time=  14.4s\n",
      "[CV] END criterion=gini, max_depth=7, max_features=log2, n_estimators=500; total time=  14.3s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=auto, n_estimators=200; total time=  12.4s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=auto, n_estimators=200; total time=  12.2s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=auto, n_estimators=200; total time=  12.3s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=auto, n_estimators=200; total time=  12.2s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=auto, n_estimators=200; total time=  12.3s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=auto, n_estimators=500; total time=  31.2s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=auto, n_estimators=500; total time=  30.9s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=auto, n_estimators=500; total time=  31.0s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=auto, n_estimators=500; total time=  31.5s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=auto, n_estimators=500; total time=  30.9s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=sqrt, n_estimators=200; total time=  12.3s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=sqrt, n_estimators=200; total time=  12.3s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=sqrt, n_estimators=200; total time=  12.4s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=sqrt, n_estimators=200; total time=  12.9s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=sqrt, n_estimators=200; total time=  12.6s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=sqrt, n_estimators=500; total time=  31.1s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=sqrt, n_estimators=500; total time=  30.9s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=sqrt, n_estimators=500; total time=  31.0s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=sqrt, n_estimators=500; total time=  31.0s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=sqrt, n_estimators=500; total time=  30.7s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=log2, n_estimators=200; total time=   6.4s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=log2, n_estimators=200; total time=   6.4s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=log2, n_estimators=200; total time=   6.6s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=log2, n_estimators=200; total time=   6.4s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=log2, n_estimators=200; total time=   6.5s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=log2, n_estimators=500; total time=  16.2s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=log2, n_estimators=500; total time=  16.1s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=log2, n_estimators=500; total time=  16.3s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=log2, n_estimators=500; total time=  16.2s\n",
      "[CV] END criterion=gini, max_depth=8, max_features=log2, n_estimators=500; total time=  16.3s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=200; total time=   8.2s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=200; total time=   8.1s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=200; total time=   8.1s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=200; total time=   8.1s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=200; total time=   8.2s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=500; total time=  20.5s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=500; total time=  20.4s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=500; total time=  20.5s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=500; total time=  20.6s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=500; total time=  20.5s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=200; total time=   8.0s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=200; total time=   8.1s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=200; total time=   8.1s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=200; total time=   8.0s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=200; total time=   8.0s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=500; total time=  20.0s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=500; total time=  20.0s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=500; total time=  20.1s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=500; total time=  20.0s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=500; total time=  20.2s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=200; total time=   4.2s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=200; total time=   4.1s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=200; total time=   4.2s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=200; total time=   4.2s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=200; total time=   4.1s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=500; total time=  10.5s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=500; total time=  10.4s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=500; total time=  10.5s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=500; total time=  10.4s\n",
      "[CV] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=500; total time=  10.5s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=200; total time=   9.9s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=200; total time=   9.9s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=200; total time=  10.0s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=200; total time=  10.0s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=200; total time=  10.0s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=500; total time=  25.0s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=500; total time=  25.3s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=500; total time=  25.2s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=500; total time=  25.0s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=500; total time=  25.0s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=200; total time=  10.0s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=200; total time=   9.9s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=200; total time=   9.9s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=200; total time=  10.0s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=200; total time=  10.0s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=500; total time=  25.1s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=500; total time=  25.0s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=500; total time=  25.0s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=500; total time=  25.0s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=500; total time=  24.9s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=200; total time=   5.2s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=200; total time=   5.2s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=200; total time=   5.1s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=200; total time=   5.2s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=200; total time=   5.1s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=500; total time=  13.4s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=500; total time=  13.2s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=500; total time=  12.9s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=500; total time=  13.0s\n",
      "[CV] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=500; total time=  12.9s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=auto, n_estimators=200; total time=  12.0s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=auto, n_estimators=200; total time=  12.0s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=auto, n_estimators=200; total time=  11.9s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=auto, n_estimators=200; total time=  11.9s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=auto, n_estimators=200; total time=  11.9s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=auto, n_estimators=500; total time=  30.5s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=auto, n_estimators=500; total time=  30.3s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=auto, n_estimators=500; total time=  30.1s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=auto, n_estimators=500; total time=  28.8s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=auto, n_estimators=500; total time=  28.9s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=sqrt, n_estimators=200; total time=  11.5s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=sqrt, n_estimators=200; total time=  11.5s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=sqrt, n_estimators=200; total time=  11.5s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=sqrt, n_estimators=200; total time=  11.4s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=sqrt, n_estimators=200; total time=  11.4s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=sqrt, n_estimators=500; total time=  28.9s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=sqrt, n_estimators=500; total time=  28.9s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=sqrt, n_estimators=500; total time=  28.8s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=sqrt, n_estimators=500; total time=  28.8s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=sqrt, n_estimators=500; total time=  28.8s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=log2, n_estimators=200; total time=   5.8s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=log2, n_estimators=200; total time=   5.8s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=log2, n_estimators=200; total time=   5.9s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=log2, n_estimators=200; total time=   5.9s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=log2, n_estimators=200; total time=   5.9s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=log2, n_estimators=500; total time=  14.6s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=log2, n_estimators=500; total time=  14.7s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=log2, n_estimators=500; total time=  14.8s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=log2, n_estimators=500; total time=  14.7s\n",
      "[CV] END criterion=entropy, max_depth=6, max_features=log2, n_estimators=500; total time=  14.6s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=auto, n_estimators=200; total time=  13.4s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=auto, n_estimators=200; total time=  13.4s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=auto, n_estimators=200; total time=  13.5s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=auto, n_estimators=200; total time=  13.5s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=auto, n_estimators=200; total time=  13.6s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=auto, n_estimators=500; total time=  34.5s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=auto, n_estimators=500; total time=  34.5s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=auto, n_estimators=500; total time=  34.0s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=auto, n_estimators=500; total time=  34.2s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=auto, n_estimators=500; total time=  34.4s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=sqrt, n_estimators=200; total time=  13.7s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=sqrt, n_estimators=200; total time=  13.7s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=sqrt, n_estimators=200; total time=  13.8s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=sqrt, n_estimators=200; total time=  13.7s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=sqrt, n_estimators=200; total time=  13.7s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=sqrt, n_estimators=500; total time=  34.5s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=sqrt, n_estimators=500; total time=  34.3s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=sqrt, n_estimators=500; total time=  34.6s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=sqrt, n_estimators=500; total time=  34.3s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=sqrt, n_estimators=500; total time=  34.3s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=log2, n_estimators=200; total time=   7.0s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=log2, n_estimators=200; total time=   6.9s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=log2, n_estimators=200; total time=   7.0s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=log2, n_estimators=200; total time=   6.9s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=log2, n_estimators=200; total time=   6.9s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=log2, n_estimators=500; total time=  17.5s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=log2, n_estimators=500; total time=  17.5s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=log2, n_estimators=500; total time=  17.5s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=log2, n_estimators=500; total time=  17.4s\n",
      "[CV] END criterion=entropy, max_depth=7, max_features=log2, n_estimators=500; total time=  16.9s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=auto, n_estimators=200; total time=  15.4s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=auto, n_estimators=200; total time=  15.3s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=auto, n_estimators=200; total time=  15.5s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=auto, n_estimators=200; total time=  15.3s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=auto, n_estimators=200; total time=  15.3s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=auto, n_estimators=500; total time=  38.9s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=auto, n_estimators=500; total time=  38.6s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=auto, n_estimators=500; total time=  39.1s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=auto, n_estimators=500; total time=  38.8s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=auto, n_estimators=500; total time=  38.6s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=sqrt, n_estimators=200; total time=  15.4s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=sqrt, n_estimators=200; total time=  15.3s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=sqrt, n_estimators=200; total time=  15.4s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=sqrt, n_estimators=200; total time=  15.3s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=sqrt, n_estimators=200; total time=  15.3s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=sqrt, n_estimators=500; total time=  38.8s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=sqrt, n_estimators=500; total time=  38.6s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=sqrt, n_estimators=500; total time=  38.9s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=sqrt, n_estimators=500; total time=  39.9s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=sqrt, n_estimators=500; total time=  40.0s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=log2, n_estimators=200; total time=   8.0s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=log2, n_estimators=200; total time=   8.1s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=log2, n_estimators=200; total time=   8.1s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=log2, n_estimators=200; total time=   8.2s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=log2, n_estimators=200; total time=   8.1s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=log2, n_estimators=500; total time=  20.4s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=log2, n_estimators=500; total time=  20.2s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=log2, n_estimators=500; total time=  20.4s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=log2, n_estimators=500; total time=  20.4s\n",
      "[CV] END criterion=entropy, max_depth=8, max_features=log2, n_estimators=500; total time=  20.4s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.575270</td>\n",
       "      <td>0.038849</td>\n",
       "      <td>0.042323</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 4, 'max_fea...</td>\n",
       "      <td>0.5926</td>\n",
       "      <td>0.5678</td>\n",
       "      <td>0.5734</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.5848</td>\n",
       "      <td>0.57672</td>\n",
       "      <td>0.010444</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.311937</td>\n",
       "      <td>0.110450</td>\n",
       "      <td>0.098235</td>\n",
       "      <td>0.001620</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 4, 'max_fea...</td>\n",
       "      <td>0.5922</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.5732</td>\n",
       "      <td>0.5644</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>0.57716</td>\n",
       "      <td>0.012199</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.453995</td>\n",
       "      <td>0.084929</td>\n",
       "      <td>0.040492</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 4, 'max_fea...</td>\n",
       "      <td>0.5926</td>\n",
       "      <td>0.5678</td>\n",
       "      <td>0.5734</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.5848</td>\n",
       "      <td>0.57672</td>\n",
       "      <td>0.010444</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.221031</td>\n",
       "      <td>0.199385</td>\n",
       "      <td>0.098311</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 4, 'max_fea...</td>\n",
       "      <td>0.5922</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.5732</td>\n",
       "      <td>0.5644</td>\n",
       "      <td>0.5910</td>\n",
       "      <td>0.57716</td>\n",
       "      <td>0.012199</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.563520</td>\n",
       "      <td>0.063343</td>\n",
       "      <td>0.042023</td>\n",
       "      <td>0.003287</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 4, 'max_fea...</td>\n",
       "      <td>0.5796</td>\n",
       "      <td>0.5634</td>\n",
       "      <td>0.5694</td>\n",
       "      <td>0.5496</td>\n",
       "      <td>0.5788</td>\n",
       "      <td>0.56816</td>\n",
       "      <td>0.011070</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.747246</td>\n",
       "      <td>0.038543</td>\n",
       "      <td>0.100681</td>\n",
       "      <td>0.007626</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 4, 'max_fea...</td>\n",
       "      <td>0.5796</td>\n",
       "      <td>0.5616</td>\n",
       "      <td>0.5692</td>\n",
       "      <td>0.5558</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.56944</td>\n",
       "      <td>0.009843</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.988752</td>\n",
       "      <td>0.038513</td>\n",
       "      <td>0.043532</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'max_fea...</td>\n",
       "      <td>0.5946</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>0.5776</td>\n",
       "      <td>0.5676</td>\n",
       "      <td>0.5848</td>\n",
       "      <td>0.57892</td>\n",
       "      <td>0.009898</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19.954882</td>\n",
       "      <td>0.047568</td>\n",
       "      <td>0.106720</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'max_fea...</td>\n",
       "      <td>0.5926</td>\n",
       "      <td>0.5692</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.5894</td>\n",
       "      <td>0.57880</td>\n",
       "      <td>0.010247</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.007312</td>\n",
       "      <td>0.062574</td>\n",
       "      <td>0.043687</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'max_fea...</td>\n",
       "      <td>0.5946</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>0.5776</td>\n",
       "      <td>0.5676</td>\n",
       "      <td>0.5848</td>\n",
       "      <td>0.57892</td>\n",
       "      <td>0.009898</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19.999018</td>\n",
       "      <td>0.047740</td>\n",
       "      <td>0.106853</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'max_fea...</td>\n",
       "      <td>0.5926</td>\n",
       "      <td>0.5692</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.5894</td>\n",
       "      <td>0.57880</td>\n",
       "      <td>0.010247</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.259787</td>\n",
       "      <td>0.016341</td>\n",
       "      <td>0.041956</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'max_fea...</td>\n",
       "      <td>0.5822</td>\n",
       "      <td>0.5696</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>0.5620</td>\n",
       "      <td>0.5778</td>\n",
       "      <td>0.57232</td>\n",
       "      <td>0.007027</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.611083</td>\n",
       "      <td>0.069678</td>\n",
       "      <td>0.103199</td>\n",
       "      <td>0.001849</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'max_fea...</td>\n",
       "      <td>0.5794</td>\n",
       "      <td>0.5680</td>\n",
       "      <td>0.5704</td>\n",
       "      <td>0.5614</td>\n",
       "      <td>0.5836</td>\n",
       "      <td>0.57256</td>\n",
       "      <td>0.007983</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9.500494</td>\n",
       "      <td>0.069659</td>\n",
       "      <td>0.046864</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6, 'max_fea...</td>\n",
       "      <td>0.5976</td>\n",
       "      <td>0.5736</td>\n",
       "      <td>0.5768</td>\n",
       "      <td>0.5672</td>\n",
       "      <td>0.5922</td>\n",
       "      <td>0.58148</td>\n",
       "      <td>0.011512</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>23.683338</td>\n",
       "      <td>0.053946</td>\n",
       "      <td>0.114080</td>\n",
       "      <td>0.002677</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6, 'max_fea...</td>\n",
       "      <td>0.5974</td>\n",
       "      <td>0.5742</td>\n",
       "      <td>0.5760</td>\n",
       "      <td>0.5670</td>\n",
       "      <td>0.5922</td>\n",
       "      <td>0.58136</td>\n",
       "      <td>0.011498</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9.649728</td>\n",
       "      <td>0.210648</td>\n",
       "      <td>0.057264</td>\n",
       "      <td>0.013395</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6, 'max_fea...</td>\n",
       "      <td>0.5976</td>\n",
       "      <td>0.5736</td>\n",
       "      <td>0.5768</td>\n",
       "      <td>0.5672</td>\n",
       "      <td>0.5922</td>\n",
       "      <td>0.58148</td>\n",
       "      <td>0.011512</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23.910204</td>\n",
       "      <td>0.286217</td>\n",
       "      <td>0.115229</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6, 'max_fea...</td>\n",
       "      <td>0.5974</td>\n",
       "      <td>0.5742</td>\n",
       "      <td>0.5760</td>\n",
       "      <td>0.5670</td>\n",
       "      <td>0.5922</td>\n",
       "      <td>0.58136</td>\n",
       "      <td>0.011498</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.991967</td>\n",
       "      <td>0.016726</td>\n",
       "      <td>0.045722</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6, 'max_fea...</td>\n",
       "      <td>0.5816</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.5688</td>\n",
       "      <td>0.5626</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.57184</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12.490435</td>\n",
       "      <td>0.032614</td>\n",
       "      <td>0.108941</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6, 'max_fea...</td>\n",
       "      <td>0.5824</td>\n",
       "      <td>0.5674</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.5634</td>\n",
       "      <td>0.5834</td>\n",
       "      <td>0.57424</td>\n",
       "      <td>0.007936</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.997593</td>\n",
       "      <td>0.074288</td>\n",
       "      <td>0.048970</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>gini</td>\n",
       "      <td>7</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 7, 'max_fea...</td>\n",
       "      <td>0.5984</td>\n",
       "      <td>0.5756</td>\n",
       "      <td>0.5712</td>\n",
       "      <td>0.5696</td>\n",
       "      <td>0.5976</td>\n",
       "      <td>0.58248</td>\n",
       "      <td>0.012826</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>27.264823</td>\n",
       "      <td>0.247573</td>\n",
       "      <td>0.119060</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>gini</td>\n",
       "      <td>7</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 7, 'max_fea...</td>\n",
       "      <td>0.5970</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.5752</td>\n",
       "      <td>0.5678</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>0.58288</td>\n",
       "      <td>0.012812</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.782152</td>\n",
       "      <td>0.194841</td>\n",
       "      <td>0.049517</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>gini</td>\n",
       "      <td>7</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 7, 'max_fea...</td>\n",
       "      <td>0.5984</td>\n",
       "      <td>0.5756</td>\n",
       "      <td>0.5712</td>\n",
       "      <td>0.5696</td>\n",
       "      <td>0.5976</td>\n",
       "      <td>0.58248</td>\n",
       "      <td>0.012826</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>27.336844</td>\n",
       "      <td>0.102489</td>\n",
       "      <td>0.120500</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>gini</td>\n",
       "      <td>7</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 7, 'max_fea...</td>\n",
       "      <td>0.5970</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.5752</td>\n",
       "      <td>0.5678</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>0.58288</td>\n",
       "      <td>0.012812</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.769280</td>\n",
       "      <td>0.023771</td>\n",
       "      <td>0.049732</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>gini</td>\n",
       "      <td>7</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 7, 'max_fea...</td>\n",
       "      <td>0.5882</td>\n",
       "      <td>0.5734</td>\n",
       "      <td>0.5666</td>\n",
       "      <td>0.5610</td>\n",
       "      <td>0.5898</td>\n",
       "      <td>0.57580</td>\n",
       "      <td>0.011482</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.342583</td>\n",
       "      <td>0.068127</td>\n",
       "      <td>0.116263</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>gini</td>\n",
       "      <td>7</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 7, 'max_fea...</td>\n",
       "      <td>0.5840</td>\n",
       "      <td>0.5708</td>\n",
       "      <td>0.5728</td>\n",
       "      <td>0.5636</td>\n",
       "      <td>0.5896</td>\n",
       "      <td>0.57616</td>\n",
       "      <td>0.009379</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>12.354456</td>\n",
       "      <td>0.079276</td>\n",
       "      <td>0.054361</td>\n",
       "      <td>0.005986</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 8, 'max_fea...</td>\n",
       "      <td>0.5968</td>\n",
       "      <td>0.5726</td>\n",
       "      <td>0.5748</td>\n",
       "      <td>0.5704</td>\n",
       "      <td>0.5932</td>\n",
       "      <td>0.58156</td>\n",
       "      <td>0.011120</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>31.065175</td>\n",
       "      <td>0.211341</td>\n",
       "      <td>0.127784</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 8, 'max_fea...</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.5752</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.5736</td>\n",
       "      <td>0.5978</td>\n",
       "      <td>0.58352</td>\n",
       "      <td>0.011109</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12.562331</td>\n",
       "      <td>0.239057</td>\n",
       "      <td>0.054118</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 8, 'max_fea...</td>\n",
       "      <td>0.5968</td>\n",
       "      <td>0.5726</td>\n",
       "      <td>0.5748</td>\n",
       "      <td>0.5704</td>\n",
       "      <td>0.5932</td>\n",
       "      <td>0.58156</td>\n",
       "      <td>0.011120</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>30.918171</td>\n",
       "      <td>0.156854</td>\n",
       "      <td>0.125922</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 8, 'max_fea...</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.5752</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.5736</td>\n",
       "      <td>0.5978</td>\n",
       "      <td>0.58352</td>\n",
       "      <td>0.011109</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6.504834</td>\n",
       "      <td>0.066172</td>\n",
       "      <td>0.050999</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 8, 'max_fea...</td>\n",
       "      <td>0.5924</td>\n",
       "      <td>0.5754</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>0.57776</td>\n",
       "      <td>0.010472</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>16.183147</td>\n",
       "      <td>0.086501</td>\n",
       "      <td>0.123946</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 8, 'max_fea...</td>\n",
       "      <td>0.5870</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.5712</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.5936</td>\n",
       "      <td>0.57836</td>\n",
       "      <td>0.010469</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.208119</td>\n",
       "      <td>0.022159</td>\n",
       "      <td>0.041382</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 4, 'max_...</td>\n",
       "      <td>0.5890</td>\n",
       "      <td>0.5664</td>\n",
       "      <td>0.5748</td>\n",
       "      <td>0.5696</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>0.57740</td>\n",
       "      <td>0.009156</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>20.503876</td>\n",
       "      <td>0.071126</td>\n",
       "      <td>0.099180</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 4, 'max_...</td>\n",
       "      <td>0.5908</td>\n",
       "      <td>0.5664</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.5672</td>\n",
       "      <td>0.5892</td>\n",
       "      <td>0.57732</td>\n",
       "      <td>0.010613</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8.090103</td>\n",
       "      <td>0.048416</td>\n",
       "      <td>0.040892</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 4, 'max_...</td>\n",
       "      <td>0.5890</td>\n",
       "      <td>0.5664</td>\n",
       "      <td>0.5748</td>\n",
       "      <td>0.5696</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>0.57740</td>\n",
       "      <td>0.009156</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>20.052544</td>\n",
       "      <td>0.070635</td>\n",
       "      <td>0.099683</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 4, 'max_...</td>\n",
       "      <td>0.5908</td>\n",
       "      <td>0.5664</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.5672</td>\n",
       "      <td>0.5892</td>\n",
       "      <td>0.57732</td>\n",
       "      <td>0.010613</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4.224851</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.039776</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 4, 'max_...</td>\n",
       "      <td>0.5804</td>\n",
       "      <td>0.5654</td>\n",
       "      <td>0.5698</td>\n",
       "      <td>0.5528</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>0.56968</td>\n",
       "      <td>0.010244</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10.463175</td>\n",
       "      <td>0.021233</td>\n",
       "      <td>0.095970</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 4, 'max_...</td>\n",
       "      <td>0.5804</td>\n",
       "      <td>0.5646</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.5582</td>\n",
       "      <td>0.5802</td>\n",
       "      <td>0.57040</td>\n",
       "      <td>0.008738</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10.010759</td>\n",
       "      <td>0.027021</td>\n",
       "      <td>0.043415</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'max_...</td>\n",
       "      <td>0.5948</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.5786</td>\n",
       "      <td>0.5672</td>\n",
       "      <td>0.5858</td>\n",
       "      <td>0.57892</td>\n",
       "      <td>0.010506</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>25.088622</td>\n",
       "      <td>0.109821</td>\n",
       "      <td>0.104384</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'max_...</td>\n",
       "      <td>0.5920</td>\n",
       "      <td>0.5708</td>\n",
       "      <td>0.5762</td>\n",
       "      <td>0.5696</td>\n",
       "      <td>0.5892</td>\n",
       "      <td>0.57956</td>\n",
       "      <td>0.009326</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>10.013515</td>\n",
       "      <td>0.047128</td>\n",
       "      <td>0.044071</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'max_...</td>\n",
       "      <td>0.5948</td>\n",
       "      <td>0.5682</td>\n",
       "      <td>0.5786</td>\n",
       "      <td>0.5672</td>\n",
       "      <td>0.5858</td>\n",
       "      <td>0.57892</td>\n",
       "      <td>0.010506</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>24.974781</td>\n",
       "      <td>0.061680</td>\n",
       "      <td>0.104932</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'max_...</td>\n",
       "      <td>0.5920</td>\n",
       "      <td>0.5708</td>\n",
       "      <td>0.5762</td>\n",
       "      <td>0.5696</td>\n",
       "      <td>0.5892</td>\n",
       "      <td>0.57956</td>\n",
       "      <td>0.009326</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5.202881</td>\n",
       "      <td>0.017321</td>\n",
       "      <td>0.043516</td>\n",
       "      <td>0.002782</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'max_...</td>\n",
       "      <td>0.5850</td>\n",
       "      <td>0.5688</td>\n",
       "      <td>0.5684</td>\n",
       "      <td>0.5620</td>\n",
       "      <td>0.5836</td>\n",
       "      <td>0.57356</td>\n",
       "      <td>0.009106</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>13.075293</td>\n",
       "      <td>0.176188</td>\n",
       "      <td>0.102054</td>\n",
       "      <td>0.002059</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'max_...</td>\n",
       "      <td>0.5802</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>0.5688</td>\n",
       "      <td>0.5636</td>\n",
       "      <td>0.5812</td>\n",
       "      <td>0.57276</td>\n",
       "      <td>0.006838</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>12.008024</td>\n",
       "      <td>0.050015</td>\n",
       "      <td>0.044641</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 6, 'max_...</td>\n",
       "      <td>0.5958</td>\n",
       "      <td>0.5748</td>\n",
       "      <td>0.5758</td>\n",
       "      <td>0.5680</td>\n",
       "      <td>0.5920</td>\n",
       "      <td>0.58128</td>\n",
       "      <td>0.010716</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>29.693900</td>\n",
       "      <td>0.724748</td>\n",
       "      <td>0.110287</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 6, 'max_...</td>\n",
       "      <td>0.5962</td>\n",
       "      <td>0.5754</td>\n",
       "      <td>0.5782</td>\n",
       "      <td>0.5688</td>\n",
       "      <td>0.5954</td>\n",
       "      <td>0.58280</td>\n",
       "      <td>0.011048</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>11.520622</td>\n",
       "      <td>0.025551</td>\n",
       "      <td>0.044627</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 6, 'max_...</td>\n",
       "      <td>0.5958</td>\n",
       "      <td>0.5748</td>\n",
       "      <td>0.5758</td>\n",
       "      <td>0.5680</td>\n",
       "      <td>0.5920</td>\n",
       "      <td>0.58128</td>\n",
       "      <td>0.010716</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>28.812778</td>\n",
       "      <td>0.055305</td>\n",
       "      <td>0.108216</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 6, 'max_...</td>\n",
       "      <td>0.5962</td>\n",
       "      <td>0.5754</td>\n",
       "      <td>0.5782</td>\n",
       "      <td>0.5688</td>\n",
       "      <td>0.5954</td>\n",
       "      <td>0.58280</td>\n",
       "      <td>0.011048</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.920390</td>\n",
       "      <td>0.049735</td>\n",
       "      <td>0.043873</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 6, 'max_...</td>\n",
       "      <td>0.5860</td>\n",
       "      <td>0.5692</td>\n",
       "      <td>0.5708</td>\n",
       "      <td>0.5638</td>\n",
       "      <td>0.5850</td>\n",
       "      <td>0.57496</td>\n",
       "      <td>0.008919</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>14.687923</td>\n",
       "      <td>0.067006</td>\n",
       "      <td>0.104239</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 6, 'max_...</td>\n",
       "      <td>0.5826</td>\n",
       "      <td>0.5684</td>\n",
       "      <td>0.5702</td>\n",
       "      <td>0.5602</td>\n",
       "      <td>0.5888</td>\n",
       "      <td>0.57404</td>\n",
       "      <td>0.010288</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>13.533702</td>\n",
       "      <td>0.045560</td>\n",
       "      <td>0.046883</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>entropy</td>\n",
       "      <td>7</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 7, 'max_...</td>\n",
       "      <td>0.5968</td>\n",
       "      <td>0.5762</td>\n",
       "      <td>0.5772</td>\n",
       "      <td>0.5736</td>\n",
       "      <td>0.5944</td>\n",
       "      <td>0.58364</td>\n",
       "      <td>0.009865</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>34.286048</td>\n",
       "      <td>0.191439</td>\n",
       "      <td>0.115955</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>entropy</td>\n",
       "      <td>7</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 7, 'max_...</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.5758</td>\n",
       "      <td>0.5766</td>\n",
       "      <td>0.5698</td>\n",
       "      <td>0.5962</td>\n",
       "      <td>0.58368</td>\n",
       "      <td>0.012066</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>13.768324</td>\n",
       "      <td>0.035439</td>\n",
       "      <td>0.047628</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>entropy</td>\n",
       "      <td>7</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 7, 'max_...</td>\n",
       "      <td>0.5968</td>\n",
       "      <td>0.5762</td>\n",
       "      <td>0.5772</td>\n",
       "      <td>0.5736</td>\n",
       "      <td>0.5944</td>\n",
       "      <td>0.58364</td>\n",
       "      <td>0.009865</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>34.400671</td>\n",
       "      <td>0.104676</td>\n",
       "      <td>0.115670</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>entropy</td>\n",
       "      <td>7</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 7, 'max_...</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.5758</td>\n",
       "      <td>0.5766</td>\n",
       "      <td>0.5698</td>\n",
       "      <td>0.5962</td>\n",
       "      <td>0.58368</td>\n",
       "      <td>0.012066</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6.999951</td>\n",
       "      <td>0.028463</td>\n",
       "      <td>0.045862</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>entropy</td>\n",
       "      <td>7</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 7, 'max_...</td>\n",
       "      <td>0.5862</td>\n",
       "      <td>0.5738</td>\n",
       "      <td>0.5718</td>\n",
       "      <td>0.5632</td>\n",
       "      <td>0.5894</td>\n",
       "      <td>0.57688</td>\n",
       "      <td>0.009655</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>17.351603</td>\n",
       "      <td>0.213635</td>\n",
       "      <td>0.112910</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>entropy</td>\n",
       "      <td>7</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 7, 'max_...</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.5744</td>\n",
       "      <td>0.5668</td>\n",
       "      <td>0.5890</td>\n",
       "      <td>0.57752</td>\n",
       "      <td>0.008052</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>15.421098</td>\n",
       "      <td>0.059077</td>\n",
       "      <td>0.049451</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 8, 'max_...</td>\n",
       "      <td>0.5980</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.5698</td>\n",
       "      <td>0.5716</td>\n",
       "      <td>0.6010</td>\n",
       "      <td>0.58300</td>\n",
       "      <td>0.013592</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>38.772599</td>\n",
       "      <td>0.166721</td>\n",
       "      <td>0.119451</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 8, 'max_...</td>\n",
       "      <td>0.5984</td>\n",
       "      <td>0.5744</td>\n",
       "      <td>0.5716</td>\n",
       "      <td>0.5710</td>\n",
       "      <td>0.5992</td>\n",
       "      <td>0.58292</td>\n",
       "      <td>0.013019</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>15.398610</td>\n",
       "      <td>0.037443</td>\n",
       "      <td>0.048967</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 8, 'max_...</td>\n",
       "      <td>0.5980</td>\n",
       "      <td>0.5746</td>\n",
       "      <td>0.5698</td>\n",
       "      <td>0.5716</td>\n",
       "      <td>0.6010</td>\n",
       "      <td>0.58300</td>\n",
       "      <td>0.013592</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>39.201905</td>\n",
       "      <td>0.578252</td>\n",
       "      <td>0.123868</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 8, 'max_...</td>\n",
       "      <td>0.5984</td>\n",
       "      <td>0.5744</td>\n",
       "      <td>0.5716</td>\n",
       "      <td>0.5710</td>\n",
       "      <td>0.5992</td>\n",
       "      <td>0.58292</td>\n",
       "      <td>0.013019</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>8.159955</td>\n",
       "      <td>0.073389</td>\n",
       "      <td>0.049826</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 8, 'max_...</td>\n",
       "      <td>0.5882</td>\n",
       "      <td>0.5712</td>\n",
       "      <td>0.5708</td>\n",
       "      <td>0.5668</td>\n",
       "      <td>0.5894</td>\n",
       "      <td>0.57728</td>\n",
       "      <td>0.009539</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>20.342477</td>\n",
       "      <td>0.074094</td>\n",
       "      <td>0.119576</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 8, 'max_...</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>0.5704</td>\n",
       "      <td>0.5744</td>\n",
       "      <td>0.5698</td>\n",
       "      <td>0.5914</td>\n",
       "      <td>0.57864</td>\n",
       "      <td>0.008946</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        6.575270      0.038849         0.042323        0.003048   \n",
       "1       16.311937      0.110450         0.098235        0.001620   \n",
       "2        6.453995      0.084929         0.040492        0.000572   \n",
       "3       16.221031      0.199385         0.098311        0.002058   \n",
       "4        3.563520      0.063343         0.042023        0.003287   \n",
       "5        8.747246      0.038543         0.100681        0.007626   \n",
       "6        7.988752      0.038513         0.043532        0.001477   \n",
       "7       19.954882      0.047568         0.106720        0.002630   \n",
       "8        8.007312      0.062574         0.043687        0.000988   \n",
       "9       19.999018      0.047740         0.106853        0.001324   \n",
       "10       4.259787      0.016341         0.041956        0.000375   \n",
       "11      10.611083      0.069678         0.103199        0.001849   \n",
       "12       9.500494      0.069659         0.046864        0.001559   \n",
       "13      23.683338      0.053946         0.114080        0.002677   \n",
       "14       9.649728      0.210648         0.057264        0.013395   \n",
       "15      23.910204      0.286217         0.115229        0.004345   \n",
       "16       4.991967      0.016726         0.045722        0.001505   \n",
       "17      12.490435      0.032614         0.108941        0.000699   \n",
       "18      10.997593      0.074288         0.048970        0.000868   \n",
       "19      27.264823      0.247573         0.119060        0.000596   \n",
       "20      10.782152      0.194841         0.049517        0.001200   \n",
       "21      27.336844      0.102489         0.120500        0.002123   \n",
       "22       5.769280      0.023771         0.049732        0.004182   \n",
       "23      14.342583      0.068127         0.116263        0.001732   \n",
       "24      12.354456      0.079276         0.054361        0.005986   \n",
       "25      31.065175      0.211341         0.127784        0.002813   \n",
       "26      12.562331      0.239057         0.054118        0.003426   \n",
       "27      30.918171      0.156854         0.125922        0.001871   \n",
       "28       6.504834      0.066172         0.050999        0.000557   \n",
       "29      16.183147      0.086501         0.123946        0.001722   \n",
       "30       8.208119      0.022159         0.041382        0.001024   \n",
       "31      20.503876      0.071126         0.099180        0.000839   \n",
       "32       8.090103      0.048416         0.040892        0.000464   \n",
       "33      20.052544      0.070635         0.099683        0.001838   \n",
       "34       4.224851      0.028300         0.039776        0.000530   \n",
       "35      10.463175      0.021233         0.095970        0.001317   \n",
       "36      10.010759      0.027021         0.043415        0.001150   \n",
       "37      25.088622      0.109821         0.104384        0.001487   \n",
       "38      10.013515      0.047128         0.044071        0.000768   \n",
       "39      24.974781      0.061680         0.104932        0.001846   \n",
       "40       5.202881      0.017321         0.043516        0.002782   \n",
       "41      13.075293      0.176188         0.102054        0.002059   \n",
       "42      12.008024      0.050015         0.044641        0.000393   \n",
       "43      29.693900      0.724748         0.110287        0.002245   \n",
       "44      11.520622      0.025551         0.044627        0.001199   \n",
       "45      28.812778      0.055305         0.108216        0.001597   \n",
       "46       5.920390      0.049735         0.043873        0.000870   \n",
       "47      14.687923      0.067006         0.104239        0.001227   \n",
       "48      13.533702      0.045560         0.046883        0.001071   \n",
       "49      34.286048      0.191439         0.115955        0.001346   \n",
       "50      13.768324      0.035439         0.047628        0.001141   \n",
       "51      34.400671      0.104676         0.115670        0.000991   \n",
       "52       6.999951      0.028463         0.045862        0.000387   \n",
       "53      17.351603      0.213635         0.112910        0.002352   \n",
       "54      15.421098      0.059077         0.049451        0.000758   \n",
       "55      38.772599      0.166721         0.119451        0.000820   \n",
       "56      15.398610      0.037443         0.048967        0.000381   \n",
       "57      39.201905      0.578252         0.123868        0.002185   \n",
       "58       8.159955      0.073389         0.049826        0.001040   \n",
       "59      20.342477      0.074094         0.119576        0.002367   \n",
       "\n",
       "   param_criterion param_max_depth param_max_features param_n_estimators  \\\n",
       "0             gini               4               auto                200   \n",
       "1             gini               4               auto                500   \n",
       "2             gini               4               sqrt                200   \n",
       "3             gini               4               sqrt                500   \n",
       "4             gini               4               log2                200   \n",
       "5             gini               4               log2                500   \n",
       "6             gini               5               auto                200   \n",
       "7             gini               5               auto                500   \n",
       "8             gini               5               sqrt                200   \n",
       "9             gini               5               sqrt                500   \n",
       "10            gini               5               log2                200   \n",
       "11            gini               5               log2                500   \n",
       "12            gini               6               auto                200   \n",
       "13            gini               6               auto                500   \n",
       "14            gini               6               sqrt                200   \n",
       "15            gini               6               sqrt                500   \n",
       "16            gini               6               log2                200   \n",
       "17            gini               6               log2                500   \n",
       "18            gini               7               auto                200   \n",
       "19            gini               7               auto                500   \n",
       "20            gini               7               sqrt                200   \n",
       "21            gini               7               sqrt                500   \n",
       "22            gini               7               log2                200   \n",
       "23            gini               7               log2                500   \n",
       "24            gini               8               auto                200   \n",
       "25            gini               8               auto                500   \n",
       "26            gini               8               sqrt                200   \n",
       "27            gini               8               sqrt                500   \n",
       "28            gini               8               log2                200   \n",
       "29            gini               8               log2                500   \n",
       "30         entropy               4               auto                200   \n",
       "31         entropy               4               auto                500   \n",
       "32         entropy               4               sqrt                200   \n",
       "33         entropy               4               sqrt                500   \n",
       "34         entropy               4               log2                200   \n",
       "35         entropy               4               log2                500   \n",
       "36         entropy               5               auto                200   \n",
       "37         entropy               5               auto                500   \n",
       "38         entropy               5               sqrt                200   \n",
       "39         entropy               5               sqrt                500   \n",
       "40         entropy               5               log2                200   \n",
       "41         entropy               5               log2                500   \n",
       "42         entropy               6               auto                200   \n",
       "43         entropy               6               auto                500   \n",
       "44         entropy               6               sqrt                200   \n",
       "45         entropy               6               sqrt                500   \n",
       "46         entropy               6               log2                200   \n",
       "47         entropy               6               log2                500   \n",
       "48         entropy               7               auto                200   \n",
       "49         entropy               7               auto                500   \n",
       "50         entropy               7               sqrt                200   \n",
       "51         entropy               7               sqrt                500   \n",
       "52         entropy               7               log2                200   \n",
       "53         entropy               7               log2                500   \n",
       "54         entropy               8               auto                200   \n",
       "55         entropy               8               auto                500   \n",
       "56         entropy               8               sqrt                200   \n",
       "57         entropy               8               sqrt                500   \n",
       "58         entropy               8               log2                200   \n",
       "59         entropy               8               log2                500   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'criterion': 'gini', 'max_depth': 4, 'max_fea...             0.5926   \n",
       "1   {'criterion': 'gini', 'max_depth': 4, 'max_fea...             0.5922   \n",
       "2   {'criterion': 'gini', 'max_depth': 4, 'max_fea...             0.5926   \n",
       "3   {'criterion': 'gini', 'max_depth': 4, 'max_fea...             0.5922   \n",
       "4   {'criterion': 'gini', 'max_depth': 4, 'max_fea...             0.5796   \n",
       "5   {'criterion': 'gini', 'max_depth': 4, 'max_fea...             0.5796   \n",
       "6   {'criterion': 'gini', 'max_depth': 5, 'max_fea...             0.5946   \n",
       "7   {'criterion': 'gini', 'max_depth': 5, 'max_fea...             0.5926   \n",
       "8   {'criterion': 'gini', 'max_depth': 5, 'max_fea...             0.5946   \n",
       "9   {'criterion': 'gini', 'max_depth': 5, 'max_fea...             0.5926   \n",
       "10  {'criterion': 'gini', 'max_depth': 5, 'max_fea...             0.5822   \n",
       "11  {'criterion': 'gini', 'max_depth': 5, 'max_fea...             0.5794   \n",
       "12  {'criterion': 'gini', 'max_depth': 6, 'max_fea...             0.5976   \n",
       "13  {'criterion': 'gini', 'max_depth': 6, 'max_fea...             0.5974   \n",
       "14  {'criterion': 'gini', 'max_depth': 6, 'max_fea...             0.5976   \n",
       "15  {'criterion': 'gini', 'max_depth': 6, 'max_fea...             0.5974   \n",
       "16  {'criterion': 'gini', 'max_depth': 6, 'max_fea...             0.5816   \n",
       "17  {'criterion': 'gini', 'max_depth': 6, 'max_fea...             0.5824   \n",
       "18  {'criterion': 'gini', 'max_depth': 7, 'max_fea...             0.5984   \n",
       "19  {'criterion': 'gini', 'max_depth': 7, 'max_fea...             0.5970   \n",
       "20  {'criterion': 'gini', 'max_depth': 7, 'max_fea...             0.5984   \n",
       "21  {'criterion': 'gini', 'max_depth': 7, 'max_fea...             0.5970   \n",
       "22  {'criterion': 'gini', 'max_depth': 7, 'max_fea...             0.5882   \n",
       "23  {'criterion': 'gini', 'max_depth': 7, 'max_fea...             0.5840   \n",
       "24  {'criterion': 'gini', 'max_depth': 8, 'max_fea...             0.5968   \n",
       "25  {'criterion': 'gini', 'max_depth': 8, 'max_fea...             0.5964   \n",
       "26  {'criterion': 'gini', 'max_depth': 8, 'max_fea...             0.5968   \n",
       "27  {'criterion': 'gini', 'max_depth': 8, 'max_fea...             0.5964   \n",
       "28  {'criterion': 'gini', 'max_depth': 8, 'max_fea...             0.5924   \n",
       "29  {'criterion': 'gini', 'max_depth': 8, 'max_fea...             0.5870   \n",
       "30  {'criterion': 'entropy', 'max_depth': 4, 'max_...             0.5890   \n",
       "31  {'criterion': 'entropy', 'max_depth': 4, 'max_...             0.5908   \n",
       "32  {'criterion': 'entropy', 'max_depth': 4, 'max_...             0.5890   \n",
       "33  {'criterion': 'entropy', 'max_depth': 4, 'max_...             0.5908   \n",
       "34  {'criterion': 'entropy', 'max_depth': 4, 'max_...             0.5804   \n",
       "35  {'criterion': 'entropy', 'max_depth': 4, 'max_...             0.5804   \n",
       "36  {'criterion': 'entropy', 'max_depth': 5, 'max_...             0.5948   \n",
       "37  {'criterion': 'entropy', 'max_depth': 5, 'max_...             0.5920   \n",
       "38  {'criterion': 'entropy', 'max_depth': 5, 'max_...             0.5948   \n",
       "39  {'criterion': 'entropy', 'max_depth': 5, 'max_...             0.5920   \n",
       "40  {'criterion': 'entropy', 'max_depth': 5, 'max_...             0.5850   \n",
       "41  {'criterion': 'entropy', 'max_depth': 5, 'max_...             0.5802   \n",
       "42  {'criterion': 'entropy', 'max_depth': 6, 'max_...             0.5958   \n",
       "43  {'criterion': 'entropy', 'max_depth': 6, 'max_...             0.5962   \n",
       "44  {'criterion': 'entropy', 'max_depth': 6, 'max_...             0.5958   \n",
       "45  {'criterion': 'entropy', 'max_depth': 6, 'max_...             0.5962   \n",
       "46  {'criterion': 'entropy', 'max_depth': 6, 'max_...             0.5860   \n",
       "47  {'criterion': 'entropy', 'max_depth': 6, 'max_...             0.5826   \n",
       "48  {'criterion': 'entropy', 'max_depth': 7, 'max_...             0.5968   \n",
       "49  {'criterion': 'entropy', 'max_depth': 7, 'max_...             0.6000   \n",
       "50  {'criterion': 'entropy', 'max_depth': 7, 'max_...             0.5968   \n",
       "51  {'criterion': 'entropy', 'max_depth': 7, 'max_...             0.6000   \n",
       "52  {'criterion': 'entropy', 'max_depth': 7, 'max_...             0.5862   \n",
       "53  {'criterion': 'entropy', 'max_depth': 7, 'max_...             0.5844   \n",
       "54  {'criterion': 'entropy', 'max_depth': 8, 'max_...             0.5980   \n",
       "55  {'criterion': 'entropy', 'max_depth': 8, 'max_...             0.5984   \n",
       "56  {'criterion': 'entropy', 'max_depth': 8, 'max_...             0.5980   \n",
       "57  {'criterion': 'entropy', 'max_depth': 8, 'max_...             0.5984   \n",
       "58  {'criterion': 'entropy', 'max_depth': 8, 'max_...             0.5882   \n",
       "59  {'criterion': 'entropy', 'max_depth': 8, 'max_...             0.5872   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0              0.5678             0.5734             0.5650   \n",
       "1              0.5650             0.5732             0.5644   \n",
       "2              0.5678             0.5734             0.5650   \n",
       "3              0.5650             0.5732             0.5644   \n",
       "4              0.5634             0.5694             0.5496   \n",
       "5              0.5616             0.5692             0.5558   \n",
       "6              0.5700             0.5776             0.5676   \n",
       "7              0.5692             0.5746             0.5682   \n",
       "8              0.5700             0.5776             0.5676   \n",
       "9              0.5692             0.5746             0.5682   \n",
       "10             0.5696             0.5700             0.5620   \n",
       "11             0.5680             0.5704             0.5614   \n",
       "12             0.5736             0.5768             0.5672   \n",
       "13             0.5742             0.5760             0.5670   \n",
       "14             0.5736             0.5768             0.5672   \n",
       "15             0.5742             0.5760             0.5670   \n",
       "16             0.5682             0.5688             0.5626   \n",
       "17             0.5674             0.5746             0.5634   \n",
       "18             0.5756             0.5712             0.5696   \n",
       "19             0.5750             0.5752             0.5678   \n",
       "20             0.5756             0.5712             0.5696   \n",
       "21             0.5750             0.5752             0.5678   \n",
       "22             0.5734             0.5666             0.5610   \n",
       "23             0.5708             0.5728             0.5636   \n",
       "24             0.5726             0.5748             0.5704   \n",
       "25             0.5752             0.5746             0.5736   \n",
       "26             0.5726             0.5748             0.5704   \n",
       "27             0.5752             0.5746             0.5736   \n",
       "28             0.5754             0.5682             0.5656   \n",
       "29             0.5750             0.5712             0.5650   \n",
       "30             0.5664             0.5748             0.5696   \n",
       "31             0.5664             0.5730             0.5672   \n",
       "32             0.5664             0.5748             0.5696   \n",
       "33             0.5664             0.5730             0.5672   \n",
       "34             0.5654             0.5698             0.5528   \n",
       "35             0.5646             0.5686             0.5582   \n",
       "36             0.5682             0.5786             0.5672   \n",
       "37             0.5708             0.5762             0.5696   \n",
       "38             0.5682             0.5786             0.5672   \n",
       "39             0.5708             0.5762             0.5696   \n",
       "40             0.5688             0.5684             0.5620   \n",
       "41             0.5700             0.5688             0.5636   \n",
       "42             0.5748             0.5758             0.5680   \n",
       "43             0.5754             0.5782             0.5688   \n",
       "44             0.5748             0.5758             0.5680   \n",
       "45             0.5754             0.5782             0.5688   \n",
       "46             0.5692             0.5708             0.5638   \n",
       "47             0.5684             0.5702             0.5602   \n",
       "48             0.5762             0.5772             0.5736   \n",
       "49             0.5758             0.5766             0.5698   \n",
       "50             0.5762             0.5772             0.5736   \n",
       "51             0.5758             0.5766             0.5698   \n",
       "52             0.5738             0.5718             0.5632   \n",
       "53             0.5730             0.5744             0.5668   \n",
       "54             0.5746             0.5698             0.5716   \n",
       "55             0.5744             0.5716             0.5710   \n",
       "56             0.5746             0.5698             0.5716   \n",
       "57             0.5744             0.5716             0.5710   \n",
       "58             0.5712             0.5708             0.5668   \n",
       "59             0.5704             0.5744             0.5698   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0              0.5848          0.57672        0.010444               45  \n",
       "1              0.5910          0.57716        0.012199               42  \n",
       "2              0.5848          0.57672        0.010444               45  \n",
       "3              0.5910          0.57716        0.012199               42  \n",
       "4              0.5788          0.56816        0.011070               60  \n",
       "5              0.5810          0.56944        0.009843               59  \n",
       "6              0.5848          0.57892        0.009898               27  \n",
       "7              0.5894          0.57880        0.010247               31  \n",
       "8              0.5848          0.57892        0.009898               27  \n",
       "9              0.5894          0.57880        0.010247               31  \n",
       "10             0.5778          0.57232        0.007027               55  \n",
       "11             0.5836          0.57256        0.007983               54  \n",
       "12             0.5922          0.58148        0.011512               19  \n",
       "13             0.5922          0.58136        0.011498               21  \n",
       "14             0.5922          0.58148        0.011512               19  \n",
       "15             0.5922          0.58136        0.011498               21  \n",
       "16             0.5780          0.57184        0.006944               56  \n",
       "17             0.5834          0.57424        0.007936               50  \n",
       "18             0.5976          0.58248        0.012826               15  \n",
       "19             0.5994          0.58288        0.012812               11  \n",
       "20             0.5976          0.58248        0.012826               15  \n",
       "21             0.5994          0.58288        0.012812               11  \n",
       "22             0.5898          0.57580        0.011482               48  \n",
       "23             0.5896          0.57616        0.009379               47  \n",
       "24             0.5932          0.58156        0.011120               17  \n",
       "25             0.5978          0.58352        0.011109                5  \n",
       "26             0.5932          0.58156        0.011120               17  \n",
       "27             0.5978          0.58352        0.011109                5  \n",
       "28             0.5872          0.57776        0.010472               35  \n",
       "29             0.5936          0.57836        0.010469               34  \n",
       "30             0.5872          0.57740        0.009156               37  \n",
       "31             0.5892          0.57732        0.010613               39  \n",
       "32             0.5872          0.57740        0.009156               37  \n",
       "33             0.5892          0.57732        0.010613               39  \n",
       "34             0.5800          0.56968        0.010244               58  \n",
       "35             0.5802          0.57040        0.008738               57  \n",
       "36             0.5858          0.57892        0.010506               27  \n",
       "37             0.5892          0.57956        0.009326               25  \n",
       "38             0.5858          0.57892        0.010506               27  \n",
       "39             0.5892          0.57956        0.009326               25  \n",
       "40             0.5836          0.57356        0.009106               52  \n",
       "41             0.5812          0.57276        0.006838               53  \n",
       "42             0.5920          0.58128        0.010716               23  \n",
       "43             0.5954          0.58280        0.011048               13  \n",
       "44             0.5920          0.58128        0.010716               23  \n",
       "45             0.5954          0.58280        0.011048               13  \n",
       "46             0.5850          0.57496        0.008919               49  \n",
       "47             0.5888          0.57404        0.010288               51  \n",
       "48             0.5944          0.58364        0.009865                3  \n",
       "49             0.5962          0.58368        0.012066                1  \n",
       "50             0.5944          0.58364        0.009865                3  \n",
       "51             0.5962          0.58368        0.012066                1  \n",
       "52             0.5894          0.57688        0.009655               44  \n",
       "53             0.5890          0.57752        0.008052               36  \n",
       "54             0.6010          0.58300        0.013592                7  \n",
       "55             0.5992          0.58292        0.013019                9  \n",
       "56             0.6010          0.58300        0.013592                7  \n",
       "57             0.5992          0.58292        0.013019                9  \n",
       "58             0.5894          0.57728        0.009539               41  \n",
       "59             0.5914          0.57864        0.008946               33  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "grid = GridSearchCV(RandomForestClassifier(max_features=200, n_estimators=300, random_state=42),param_grid, cv= 5,verbose=2)\n",
    "grid.fit(X_train,y_train)\n",
    "display(pd.DataFrame(grid.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[CV] END ......................gamma=1, learning_rate=0.0001; total time=   5.6s\n",
      "[CV] END ......................gamma=1, learning_rate=0.0001; total time=   5.6s\n",
      "[CV] END ......................gamma=1, learning_rate=0.0001; total time=   5.7s\n",
      "[CV] END ......................gamma=1, learning_rate=0.0001; total time=   6.1s\n",
      "[CV] END ......................gamma=1, learning_rate=0.0001; total time=   6.1s\n",
      "[CV] END .......................gamma=1, learning_rate=0.001; total time=   5.8s\n",
      "[CV] END .......................gamma=1, learning_rate=0.001; total time=   5.8s\n",
      "[CV] END .......................gamma=1, learning_rate=0.001; total time=   5.7s\n",
      "[CV] END .......................gamma=1, learning_rate=0.001; total time=   5.9s\n",
      "[CV] END .......................gamma=1, learning_rate=0.001; total time=   5.7s\n",
      "[CV] END ........................gamma=1, learning_rate=0.01; total time=   5.8s\n",
      "[CV] END ........................gamma=1, learning_rate=0.01; total time=   5.9s\n",
      "[CV] END ........................gamma=1, learning_rate=0.01; total time=   5.8s\n",
      "[CV] END ........................gamma=1, learning_rate=0.01; total time=   5.7s\n",
      "[CV] END ........................gamma=1, learning_rate=0.01; total time=   5.7s\n",
      "[CV] END ........................gamma=1, learning_rate=0.01; total time=   5.6s\n",
      "[CV] END ........................gamma=1, learning_rate=0.01; total time=   5.6s\n",
      "[CV] END ........................gamma=1, learning_rate=0.01; total time=   5.6s\n",
      "[CV] END ........................gamma=1, learning_rate=0.01; total time=   5.8s\n",
      "[CV] END ........................gamma=1, learning_rate=0.01; total time=   5.8s\n",
      "[CV] END ........................gamma=1, learning_rate=0.02; total time=   5.5s\n",
      "[CV] END ........................gamma=1, learning_rate=0.02; total time=   5.8s\n",
      "[CV] END ........................gamma=1, learning_rate=0.02; total time=   5.7s\n",
      "[CV] END ........................gamma=1, learning_rate=0.02; total time=  19.8s\n",
      "[CV] END ........................gamma=1, learning_rate=0.02; total time=   5.7s\n",
      "[CV] END ........................gamma=1, learning_rate=0.03; total time=   5.4s\n",
      "[CV] END ........................gamma=1, learning_rate=0.03; total time=   5.4s\n",
      "[CV] END ........................gamma=1, learning_rate=0.03; total time=   5.6s\n",
      "[CV] END ........................gamma=1, learning_rate=0.03; total time=   5.9s\n",
      "[CV] END ........................gamma=1, learning_rate=0.03; total time=   5.7s\n",
      "[CV] END ....................gamma=0.1, learning_rate=0.0001; total time=   5.8s\n",
      "[CV] END ....................gamma=0.1, learning_rate=0.0001; total time=   5.7s\n",
      "[CV] END ....................gamma=0.1, learning_rate=0.0001; total time=   5.9s\n",
      "[CV] END ....................gamma=0.1, learning_rate=0.0001; total time=   5.8s\n",
      "[CV] END ....................gamma=0.1, learning_rate=0.0001; total time=   5.6s\n",
      "[CV] END .....................gamma=0.1, learning_rate=0.001; total time=   5.5s\n",
      "[CV] END .....................gamma=0.1, learning_rate=0.001; total time=   5.8s\n",
      "[CV] END .....................gamma=0.1, learning_rate=0.001; total time=   5.4s\n",
      "[CV] END .....................gamma=0.1, learning_rate=0.001; total time=   5.5s\n",
      "[CV] END .....................gamma=0.1, learning_rate=0.001; total time=   5.7s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.01; total time=   5.6s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.01; total time=   5.6s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.01; total time=   5.6s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.01; total time=   5.8s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.01; total time=   6.0s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.01; total time=   6.2s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.01; total time=   5.6s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.01; total time=   5.6s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.01; total time=   5.6s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.01; total time=   5.8s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.02; total time=   5.5s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.02; total time=   5.5s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.02; total time=   5.4s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.02; total time=   5.4s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.02; total time=   5.6s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.03; total time=   5.5s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.03; total time=   6.6s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.03; total time=   5.8s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.03; total time=   5.7s\n",
      "[CV] END ......................gamma=0.1, learning_rate=0.03; total time=   5.5s\n",
      "[CV] END ...................gamma=0.01, learning_rate=0.0001; total time=   6.3s\n",
      "[CV] END ...................gamma=0.01, learning_rate=0.0001; total time=   6.0s\n",
      "[CV] END ...................gamma=0.01, learning_rate=0.0001; total time=   5.8s\n",
      "[CV] END ...................gamma=0.01, learning_rate=0.0001; total time=   5.9s\n",
      "[CV] END ...................gamma=0.01, learning_rate=0.0001; total time=   5.8s\n",
      "[CV] END ....................gamma=0.01, learning_rate=0.001; total time=   5.8s\n",
      "[CV] END ....................gamma=0.01, learning_rate=0.001; total time=   5.7s\n",
      "[CV] END ....................gamma=0.01, learning_rate=0.001; total time=   5.8s\n",
      "[CV] END ....................gamma=0.01, learning_rate=0.001; total time=   5.9s\n",
      "[CV] END ....................gamma=0.01, learning_rate=0.001; total time=   5.8s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.01; total time=   5.7s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.01; total time=   5.7s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.01; total time=   5.8s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.01; total time=   5.8s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.01; total time=   5.7s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.01; total time=   5.8s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.01; total time=   5.7s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.01; total time=   6.0s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.01; total time=   5.8s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.01; total time=   5.7s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.02; total time=   5.6s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.02; total time=   5.6s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.02; total time=   5.8s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.02; total time=   5.7s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.02; total time=   5.6s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.03; total time=   5.6s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.03; total time=   5.4s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.03; total time=   5.5s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.03; total time=   5.6s\n",
      "[CV] END .....................gamma=0.01, learning_rate=0.03; total time=   5.5s\n",
      "[CV] END ..................gamma=0.001, learning_rate=0.0001; total time=   5.8s\n",
      "[CV] END ..................gamma=0.001, learning_rate=0.0001; total time=   5.8s\n",
      "[CV] END ..................gamma=0.001, learning_rate=0.0001; total time=   6.2s\n",
      "[CV] END ..................gamma=0.001, learning_rate=0.0001; total time=   5.9s\n",
      "[CV] END ..................gamma=0.001, learning_rate=0.0001; total time=   5.8s\n",
      "[CV] END ...................gamma=0.001, learning_rate=0.001; total time=   5.7s\n",
      "[CV] END ...................gamma=0.001, learning_rate=0.001; total time=   5.7s\n",
      "[CV] END ...................gamma=0.001, learning_rate=0.001; total time=   5.9s\n",
      "[CV] END ...................gamma=0.001, learning_rate=0.001; total time=   5.8s\n",
      "[CV] END ...................gamma=0.001, learning_rate=0.001; total time=   5.8s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.01; total time=   5.7s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.01; total time=   5.9s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.01; total time=   5.8s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.01; total time=   6.0s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.01; total time=   6.0s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.01; total time=   5.7s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.01; total time=   5.7s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.01; total time=   5.5s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.01; total time=   5.7s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.01; total time=   5.9s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.02; total time=   5.6s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.02; total time=   5.5s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.02; total time=   5.6s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.02; total time=   5.5s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.02; total time=   5.7s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.03; total time=   5.3s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.03; total time=   5.5s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.03; total time=   5.4s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.03; total time=   5.5s\n",
      "[CV] END ....................gamma=0.001, learning_rate=0.03; total time=   5.3s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.915322</td>\n",
       "      <td>0.206698</td>\n",
       "      <td>0.008711</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>{'gamma': 1, 'learning_rate': 0.0001}</td>\n",
       "      <td>0.5862</td>\n",
       "      <td>0.5702</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.5782</td>\n",
       "      <td>0.57464</td>\n",
       "      <td>0.007077</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.869980</td>\n",
       "      <td>0.047628</td>\n",
       "      <td>0.008225</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'gamma': 1, 'learning_rate': 0.001}</td>\n",
       "      <td>0.5930</td>\n",
       "      <td>0.5748</td>\n",
       "      <td>0.5688</td>\n",
       "      <td>0.5724</td>\n",
       "      <td>0.5836</td>\n",
       "      <td>0.57852</td>\n",
       "      <td>0.008732</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.886803</td>\n",
       "      <td>0.093330</td>\n",
       "      <td>0.006005</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'gamma': 1, 'learning_rate': 0.01}</td>\n",
       "      <td>0.5946</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.5772</td>\n",
       "      <td>0.5732</td>\n",
       "      <td>0.5888</td>\n",
       "      <td>0.58236</td>\n",
       "      <td>0.008010</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.748846</td>\n",
       "      <td>0.116002</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'gamma': 1, 'learning_rate': 0.01}</td>\n",
       "      <td>0.5946</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.5772</td>\n",
       "      <td>0.5732</td>\n",
       "      <td>0.5888</td>\n",
       "      <td>0.58236</td>\n",
       "      <td>0.008010</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.575021</td>\n",
       "      <td>5.650741</td>\n",
       "      <td>0.007288</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>{'gamma': 1, 'learning_rate': 0.02}</td>\n",
       "      <td>0.5958</td>\n",
       "      <td>0.5838</td>\n",
       "      <td>0.5804</td>\n",
       "      <td>0.5808</td>\n",
       "      <td>0.5862</td>\n",
       "      <td>0.58540</td>\n",
       "      <td>0.005613</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.698591</td>\n",
       "      <td>0.196164</td>\n",
       "      <td>0.008411</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>{'gamma': 1, 'learning_rate': 0.03}</td>\n",
       "      <td>0.6006</td>\n",
       "      <td>0.5852</td>\n",
       "      <td>0.5784</td>\n",
       "      <td>0.5862</td>\n",
       "      <td>0.5916</td>\n",
       "      <td>0.58840</td>\n",
       "      <td>0.007405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.843362</td>\n",
       "      <td>0.104659</td>\n",
       "      <td>0.007208</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>{'gamma': 0.1, 'learning_rate': 0.0001}</td>\n",
       "      <td>0.5862</td>\n",
       "      <td>0.5702</td>\n",
       "      <td>0.5728</td>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.5782</td>\n",
       "      <td>0.57460</td>\n",
       "      <td>0.007087</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.669783</td>\n",
       "      <td>0.115006</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'gamma': 0.1, 'learning_rate': 0.001}</td>\n",
       "      <td>0.5928</td>\n",
       "      <td>0.5748</td>\n",
       "      <td>0.5692</td>\n",
       "      <td>0.5722</td>\n",
       "      <td>0.5836</td>\n",
       "      <td>0.57852</td>\n",
       "      <td>0.008607</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.830324</td>\n",
       "      <td>0.164659</td>\n",
       "      <td>0.007851</td>\n",
       "      <td>0.001683</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'gamma': 0.1, 'learning_rate': 0.01}</td>\n",
       "      <td>0.5948</td>\n",
       "      <td>0.5782</td>\n",
       "      <td>0.5754</td>\n",
       "      <td>0.5722</td>\n",
       "      <td>0.5874</td>\n",
       "      <td>0.58160</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.845590</td>\n",
       "      <td>0.227062</td>\n",
       "      <td>0.008115</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'gamma': 0.1, 'learning_rate': 0.01}</td>\n",
       "      <td>0.5948</td>\n",
       "      <td>0.5782</td>\n",
       "      <td>0.5754</td>\n",
       "      <td>0.5722</td>\n",
       "      <td>0.5874</td>\n",
       "      <td>0.58160</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.575136</td>\n",
       "      <td>0.057307</td>\n",
       "      <td>0.007909</td>\n",
       "      <td>0.001719</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>{'gamma': 0.1, 'learning_rate': 0.02}</td>\n",
       "      <td>0.5968</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>0.5762</td>\n",
       "      <td>0.5812</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>0.58572</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.910514</td>\n",
       "      <td>0.416376</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>{'gamma': 0.1, 'learning_rate': 0.03}</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>0.5846</td>\n",
       "      <td>0.5772</td>\n",
       "      <td>0.5856</td>\n",
       "      <td>0.5920</td>\n",
       "      <td>0.58776</td>\n",
       "      <td>0.007478</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.037115</td>\n",
       "      <td>0.192363</td>\n",
       "      <td>0.008112</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>{'gamma': 0.01, 'learning_rate': 0.0001}</td>\n",
       "      <td>0.5862</td>\n",
       "      <td>0.5702</td>\n",
       "      <td>0.5728</td>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.5782</td>\n",
       "      <td>0.57460</td>\n",
       "      <td>0.007087</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.899504</td>\n",
       "      <td>0.060504</td>\n",
       "      <td>0.007510</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'gamma': 0.01, 'learning_rate': 0.001}</td>\n",
       "      <td>0.5928</td>\n",
       "      <td>0.5748</td>\n",
       "      <td>0.5692</td>\n",
       "      <td>0.5722</td>\n",
       "      <td>0.5836</td>\n",
       "      <td>0.57852</td>\n",
       "      <td>0.008607</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.829803</td>\n",
       "      <td>0.030992</td>\n",
       "      <td>0.006712</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'gamma': 0.01, 'learning_rate': 0.01}</td>\n",
       "      <td>0.5956</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.5758</td>\n",
       "      <td>0.5724</td>\n",
       "      <td>0.5876</td>\n",
       "      <td>0.58188</td>\n",
       "      <td>0.008519</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.910101</td>\n",
       "      <td>0.100978</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.001457</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'gamma': 0.01, 'learning_rate': 0.01}</td>\n",
       "      <td>0.5956</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.5758</td>\n",
       "      <td>0.5724</td>\n",
       "      <td>0.5876</td>\n",
       "      <td>0.58188</td>\n",
       "      <td>0.008519</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.734136</td>\n",
       "      <td>0.075127</td>\n",
       "      <td>0.007112</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>{'gamma': 0.01, 'learning_rate': 0.02}</td>\n",
       "      <td>0.5974</td>\n",
       "      <td>0.5852</td>\n",
       "      <td>0.5776</td>\n",
       "      <td>0.5766</td>\n",
       "      <td>0.5878</td>\n",
       "      <td>0.58492</td>\n",
       "      <td>0.007575</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.629117</td>\n",
       "      <td>0.065864</td>\n",
       "      <td>0.006209</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>{'gamma': 0.01, 'learning_rate': 0.03}</td>\n",
       "      <td>0.6026</td>\n",
       "      <td>0.5836</td>\n",
       "      <td>0.5738</td>\n",
       "      <td>0.5856</td>\n",
       "      <td>0.5946</td>\n",
       "      <td>0.58804</td>\n",
       "      <td>0.009833</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.007189</td>\n",
       "      <td>0.151441</td>\n",
       "      <td>0.007934</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>{'gamma': 0.001, 'learning_rate': 0.0001}</td>\n",
       "      <td>0.5862</td>\n",
       "      <td>0.5702</td>\n",
       "      <td>0.5728</td>\n",
       "      <td>0.5656</td>\n",
       "      <td>0.5782</td>\n",
       "      <td>0.57460</td>\n",
       "      <td>0.007087</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.877205</td>\n",
       "      <td>0.056361</td>\n",
       "      <td>0.007615</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'gamma': 0.001, 'learning_rate': 0.001}</td>\n",
       "      <td>0.5928</td>\n",
       "      <td>0.5748</td>\n",
       "      <td>0.5692</td>\n",
       "      <td>0.5722</td>\n",
       "      <td>0.5836</td>\n",
       "      <td>0.57852</td>\n",
       "      <td>0.008607</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.978623</td>\n",
       "      <td>0.130532</td>\n",
       "      <td>0.008207</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'gamma': 0.001, 'learning_rate': 0.01}</td>\n",
       "      <td>0.5956</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.5758</td>\n",
       "      <td>0.5724</td>\n",
       "      <td>0.5876</td>\n",
       "      <td>0.58188</td>\n",
       "      <td>0.008519</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.795148</td>\n",
       "      <td>0.129852</td>\n",
       "      <td>0.008338</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'gamma': 0.001, 'learning_rate': 0.01}</td>\n",
       "      <td>0.5956</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>0.5758</td>\n",
       "      <td>0.5724</td>\n",
       "      <td>0.5876</td>\n",
       "      <td>0.58188</td>\n",
       "      <td>0.008519</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.659062</td>\n",
       "      <td>0.063922</td>\n",
       "      <td>0.006510</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.02</td>\n",
       "      <td>{'gamma': 0.001, 'learning_rate': 0.02}</td>\n",
       "      <td>0.5974</td>\n",
       "      <td>0.5852</td>\n",
       "      <td>0.5776</td>\n",
       "      <td>0.5766</td>\n",
       "      <td>0.5878</td>\n",
       "      <td>0.58492</td>\n",
       "      <td>0.007575</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.471634</td>\n",
       "      <td>0.088850</td>\n",
       "      <td>0.007429</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.03</td>\n",
       "      <td>{'gamma': 0.001, 'learning_rate': 0.03}</td>\n",
       "      <td>0.6024</td>\n",
       "      <td>0.5836</td>\n",
       "      <td>0.5738</td>\n",
       "      <td>0.5852</td>\n",
       "      <td>0.5946</td>\n",
       "      <td>0.58792</td>\n",
       "      <td>0.009795</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_gamma  \\\n",
       "0        5.915322      0.206698         0.008711        0.000401           1   \n",
       "1        5.869980      0.047628         0.008225        0.001217           1   \n",
       "2        5.886803      0.093330         0.006005        0.000635           1   \n",
       "3        5.748846      0.116002         0.006711        0.001439           1   \n",
       "4        8.575021      5.650741         0.007288        0.001330           1   \n",
       "5        5.698591      0.196164         0.008411        0.001282           1   \n",
       "6        5.843362      0.104659         0.007208        0.001368         0.1   \n",
       "7        5.669783      0.115006         0.007207        0.001699         0.1   \n",
       "8        5.830324      0.164659         0.007851        0.001683         0.1   \n",
       "9        5.845590      0.227062         0.008115        0.001159         0.1   \n",
       "10       5.575136      0.057307         0.007909        0.001719         0.1   \n",
       "11       5.910514      0.416376         0.006411        0.000668         0.1   \n",
       "12       6.037115      0.192363         0.008112        0.001777        0.01   \n",
       "13       5.899504      0.060504         0.007510        0.001308        0.01   \n",
       "14       5.829803      0.030992         0.006712        0.000932        0.01   \n",
       "15       5.910101      0.100978         0.007300        0.001457        0.01   \n",
       "16       5.734136      0.075127         0.007112        0.001159        0.01   \n",
       "17       5.629117      0.065864         0.006209        0.000677        0.01   \n",
       "18       6.007189      0.151441         0.007934        0.001212       0.001   \n",
       "19       5.877205      0.056361         0.007615        0.001116       0.001   \n",
       "20       5.978623      0.130532         0.008207        0.000925       0.001   \n",
       "21       5.795148      0.129852         0.008338        0.001526       0.001   \n",
       "22       5.659062      0.063922         0.006510        0.000704       0.001   \n",
       "23       5.471634      0.088850         0.007429        0.001310       0.001   \n",
       "\n",
       "   param_learning_rate                                     params  \\\n",
       "0               0.0001      {'gamma': 1, 'learning_rate': 0.0001}   \n",
       "1                0.001       {'gamma': 1, 'learning_rate': 0.001}   \n",
       "2                 0.01        {'gamma': 1, 'learning_rate': 0.01}   \n",
       "3                 0.01        {'gamma': 1, 'learning_rate': 0.01}   \n",
       "4                 0.02        {'gamma': 1, 'learning_rate': 0.02}   \n",
       "5                 0.03        {'gamma': 1, 'learning_rate': 0.03}   \n",
       "6               0.0001    {'gamma': 0.1, 'learning_rate': 0.0001}   \n",
       "7                0.001     {'gamma': 0.1, 'learning_rate': 0.001}   \n",
       "8                 0.01      {'gamma': 0.1, 'learning_rate': 0.01}   \n",
       "9                 0.01      {'gamma': 0.1, 'learning_rate': 0.01}   \n",
       "10                0.02      {'gamma': 0.1, 'learning_rate': 0.02}   \n",
       "11                0.03      {'gamma': 0.1, 'learning_rate': 0.03}   \n",
       "12              0.0001   {'gamma': 0.01, 'learning_rate': 0.0001}   \n",
       "13               0.001    {'gamma': 0.01, 'learning_rate': 0.001}   \n",
       "14                0.01     {'gamma': 0.01, 'learning_rate': 0.01}   \n",
       "15                0.01     {'gamma': 0.01, 'learning_rate': 0.01}   \n",
       "16                0.02     {'gamma': 0.01, 'learning_rate': 0.02}   \n",
       "17                0.03     {'gamma': 0.01, 'learning_rate': 0.03}   \n",
       "18              0.0001  {'gamma': 0.001, 'learning_rate': 0.0001}   \n",
       "19               0.001   {'gamma': 0.001, 'learning_rate': 0.001}   \n",
       "20                0.01    {'gamma': 0.001, 'learning_rate': 0.01}   \n",
       "21                0.01    {'gamma': 0.001, 'learning_rate': 0.01}   \n",
       "22                0.02    {'gamma': 0.001, 'learning_rate': 0.02}   \n",
       "23                0.03    {'gamma': 0.001, 'learning_rate': 0.03}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0              0.5862             0.5702             0.5730   \n",
       "1              0.5930             0.5748             0.5688   \n",
       "2              0.5946             0.5780             0.5772   \n",
       "3              0.5946             0.5780             0.5772   \n",
       "4              0.5958             0.5838             0.5804   \n",
       "5              0.6006             0.5852             0.5784   \n",
       "6              0.5862             0.5702             0.5728   \n",
       "7              0.5928             0.5748             0.5692   \n",
       "8              0.5948             0.5782             0.5754   \n",
       "9              0.5948             0.5782             0.5754   \n",
       "10             0.5968             0.5844             0.5762   \n",
       "11             0.5994             0.5846             0.5772   \n",
       "12             0.5862             0.5702             0.5728   \n",
       "13             0.5928             0.5748             0.5692   \n",
       "14             0.5956             0.5780             0.5758   \n",
       "15             0.5956             0.5780             0.5758   \n",
       "16             0.5974             0.5852             0.5776   \n",
       "17             0.6026             0.5836             0.5738   \n",
       "18             0.5862             0.5702             0.5728   \n",
       "19             0.5928             0.5748             0.5692   \n",
       "20             0.5956             0.5780             0.5758   \n",
       "21             0.5956             0.5780             0.5758   \n",
       "22             0.5974             0.5852             0.5776   \n",
       "23             0.6024             0.5836             0.5738   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0              0.5656             0.5782          0.57464        0.007077   \n",
       "1              0.5724             0.5836          0.57852        0.008732   \n",
       "2              0.5732             0.5888          0.58236        0.008010   \n",
       "3              0.5732             0.5888          0.58236        0.008010   \n",
       "4              0.5808             0.5862          0.58540        0.005613   \n",
       "5              0.5862             0.5916          0.58840        0.007405   \n",
       "6              0.5656             0.5782          0.57460        0.007087   \n",
       "7              0.5722             0.5836          0.57852        0.008607   \n",
       "8              0.5722             0.5874          0.58160        0.008322   \n",
       "9              0.5722             0.5874          0.58160        0.008322   \n",
       "10             0.5812             0.5900          0.58572        0.007126   \n",
       "11             0.5856             0.5920          0.58776        0.007478   \n",
       "12             0.5656             0.5782          0.57460        0.007087   \n",
       "13             0.5722             0.5836          0.57852        0.008607   \n",
       "14             0.5724             0.5876          0.58188        0.008519   \n",
       "15             0.5724             0.5876          0.58188        0.008519   \n",
       "16             0.5766             0.5878          0.58492        0.007575   \n",
       "17             0.5856             0.5946          0.58804        0.009833   \n",
       "18             0.5656             0.5782          0.57460        0.007087   \n",
       "19             0.5722             0.5836          0.57852        0.008607   \n",
       "20             0.5724             0.5876          0.58188        0.008519   \n",
       "21             0.5724             0.5876          0.58188        0.008519   \n",
       "22             0.5766             0.5878          0.58492        0.007575   \n",
       "23             0.5852             0.5946          0.58792        0.009795   \n",
       "\n",
       "    rank_test_score  \n",
       "0                21  \n",
       "1                17  \n",
       "2                 9  \n",
       "3                 9  \n",
       "4                 6  \n",
       "5                 1  \n",
       "6                22  \n",
       "7                17  \n",
       "8                15  \n",
       "9                15  \n",
       "10                5  \n",
       "11                4  \n",
       "12               22  \n",
       "13               17  \n",
       "14               11  \n",
       "15               11  \n",
       "16                7  \n",
       "17                2  \n",
       "18               22  \n",
       "19               17  \n",
       "20               11  \n",
       "21               11  \n",
       "22                7  \n",
       "23                3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = [0.0001, 0.001, 0.01, 0.01, 0.02, 0.03]\n",
    "gamma = [1, 0.1, 0.01, 0.001]\n",
    "param_grid = dict(learning_rate = learning_rate, gamma = gamma)\n",
    "grid = GridSearchCV(XGBClassifier(),param_grid,refit=True,verbose=2)\n",
    "grid.fit(X_train,y_train)\n",
    "display(pd.DataFrame(grid.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: Random Forest\n",
      "\n",
      "AUC Score: 0.59\n",
      "\n",
      "Recorded Time(s): 1197.57\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.61      0.59     12483\n",
      "           1       0.59      0.56      0.58     12517\n",
      "\n",
      "    accuracy                           0.59     25000\n",
      "   macro avg       0.59      0.59      0.59     25000\n",
      "weighted avg       0.59      0.59      0.59     25000\n",
      "\n",
      "\n",
      "\n",
      "Model Name: XGBoost\n",
      "\n",
      "AUC Score: 0.59\n",
      "\n",
      "Recorded Time(s): 6.08\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.62      0.60     12483\n",
      "           1       0.60      0.56      0.58     12517\n",
      "\n",
      "    accuracy                           0.59     25000\n",
      "   macro avg       0.59      0.59      0.59     25000\n",
      "weighted avg       0.59      0.59      0.59     25000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models={'Random Forest':RandomForestClassifier(max_features=200, n_estimators=300, random_state=42),\n",
    "        'XGBoost':XGBClassifier()}\n",
    "                                                            ##Random Forest max_features = sqrt(no. of features) = sqrt(28938)\n",
    "classifreport, predictions, time={}, [], []\n",
    "\n",
    "for key in models.keys():\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    models[key].fit(X_train, y_train)  \n",
    "    stop = timeit.default_timer()\n",
    "    \n",
    "    time.append(stop - start)\n",
    "    \n",
    "    prediction=models[key].predict(X_test)\n",
    "    predictions.append(prediction)\n",
    "    \n",
    "    classifreport[key]=classification_report(y_test, prediction, target_names=['0','1'])\n",
    "\n",
    "fpr1, tpr1, thresh1 = roc_curve(y_test, predictions[0], pos_label=1)\n",
    "fpr2, tpr2, thresh2 = roc_curve(y_test, predictions[1], pos_label=1)\n",
    "\n",
    "\n",
    "random_probs = [0 for i in range(len(y_test))]\n",
    "p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\n",
    "\n",
    "auc_score1 = roc_auc_score(y_test, predictions[0])\n",
    "auc_score2 = roc_auc_score(y_test, predictions[1])\n",
    "\n",
    "\n",
    "model_name=list(models.keys())\n",
    "       \n",
    "for a, b, c, k in zip(model_name,[auc_score1, auc_score2], time, classifreport.items()):\n",
    "    print(\"Model Name: \"+a+\"\\n\\n\"+\"AUC Score: {:.2f}\".format(b)+\"\\n\\n\"+\"Recorded Time(s): {:.2f}\".format(c)+\"\\n\\n\"+k[1]+\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "plt.plot(fpr1, tpr1, linestyle='--',color='orange', label=model_name[0])\n",
    "plt.plot(fpr2, tpr2, linestyle='--',color='green', label=model_name[1])\n",
    "\n",
    "plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('ROC',dpi=300)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:\n",
      "[0.56928 0.57256 0.5708  0.57016 0.57472]\n",
      "Mean accuracy: 0.57\n"
     ]
    }
   ],
   "source": [
    "stratified_shuffle_split = StratifiedShuffleSplit(test_size=.5, train_size=.5, n_splits=5)\n",
    "scores = cross_val_score(models['XGBoost'], X_train, y_train, cv=stratified_shuffle_split)\n",
    "print(\"Cross-validation scores:\\n{}\".format(scores))\n",
    "print(\"Mean accuracy: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[7757 4726]\n",
      " [5543 6974]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbIAAAFXCAYAAADH+sstAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAAsTAAALEwEAmpwYAAAkP0lEQVR4nO3df3RU9Z3/8dfM5AchE4w/sDUSEiiGEhAiRFAbqGhZFLu48jNg2baC2QKhQuSH8QAhBTGoRJekJ+WALi4ov6G6tSoF9WCAhcA3UUIIhf3yU5rgQfhCJiMhmfv9w2N200CYDVySe+/z0TPnODd37ud956Tz4v25n9xxGYZhCAAAi3K3dAEAAFwPggwAYGkEGQDA0ggyAIClEWQAAEsjyAAAlkaQOcDhw4eVlpamcePGafjw4VqyZIkMw9Du3bs1bdq0GzpWIBDQ3LlzNXr0aI0bN07Hjx+/ocdH63Azf6e+98UXX2jcuHGmHBvWFtLSBcBcFy5cUEZGhvLy8hQfH6+6ujo999xzWrNmjTp37nzDx9u6datqamq0du1alZSUKCcnRwUFBTd8HLScm/07JUnLli3T+++/r4iICFOOD2sjyGxu27Zt6tevn+Lj4yVJHo9HixYtUmhoqIqLi+v3W7VqlbZs2SK/369bb71V+fn5+uqrr5SZmamQkBAFAgEtXrxY4eHhmjp1qgzD0KVLl5Sdna1u3brVH2ffvn3q37+/JCkpKUmlpaU39Xxhvpv9OyVJHTt2VF5enmbOnHkzTxUWQZDZ3JkzZxQbG9tgW2RkZIPngUBA58+f14oVK+R2uzV+/Hjt379f5eXl6tmzp2bMmKG9e/fq4sWLOnTokKKjo/XKK6/oyJEjqq6ubnCsqqoqeb3e+ucej0e1tbUKCeFXzS5u9u+UJA0ePFinTp0y9bxgXXy62FxMTIzKysoabDt58qQqKirqn7vdboWGhiojI0Nt27ZVRUWFamtrNWLECC1btkwTJkxQVFSUpk2bpgEDBujYsWOaNGmSQkJCNHHixAbH9nq98vl89c8DgQAhZjM3+3cKuBYWe9jcwIED9fnnn+vEiROSpMuXLysnJ0d//etf6/cpLy/X1q1b9cYbb2jOnDkKBAIyDEPbtm1Tnz599Pbbb+uxxx7T8uXLtXv3bt1555166623NHHiROXm5jYYr3fv3tq+fbskqaSkRAkJCTfvZHFT3OzfKeBa+KeyzXm9XuXk5Gj27NkyDEM+n08DBw7U2LFjtWfPHklSXFycIiIilJqaKklq3769zpw5o6SkJM2aNUsFBQUKBALKzMxUTEyMMjIytHr1atXW1mry5MkNxhs0aJB27Nih1NRUGYahhQsX3vRzhrlu9u8UcC0u7n4PALAyphYBAJZGkAEALI0gAwBYGkEGALA0ggwAYGkEGQDA0ggyAIClEWQAAEtz7J09Iu5Lb+kSLGPv+heVPJI7dFzLuaL8li7BMsI8Uk1dS1dhDW1M+pS+ns9Af3Hr+l13bJAheN27xLR0CbAZt6ulK4Bc9pmQI8gAwIlc9vnXBEEGAE5ERwYAsDQbdWT2iWQAgCPRkQGAEzG1CACwNBtNLRJkAOBEdGQAAEujIwMAWBodGQDA0mzUkdknkgEAjkRHBgBOxNQiAMDSbDS1SJABgBPRkQEALI0gAwBYmo2+FM4+kQwAcCQ6MgBwIqYWAQCWxqpFAICl0ZEBACzNxI5s06ZN2rx5syTp0qVLOnjwoHJzc7Vo0SLdddddkqQpU6YoOTlZ8+bN06FDhxQWFqYFCxYoLi5OJSUleumll+TxeJSSkqL09PQmxyPIAMCJTOzIhg0bpmHDhkmSsrOzNXz4cJWWlmrGjBkaPHhw/X5btmxRTU2N1q5dq5KSEuXk5KigoEBZWVnKy8tTbGys0tLSVFZWpsTExKuOZ5/eEgAQPJer+Y8g7d+/X0eOHNHo0aN14MABbdy4UWPHjlVOTo5qa2u1b98+9e/fX5KUlJSk0tJSVVVVqaamRh07dpTL5VJKSop27tzZ5DgEGQDAFEuXLtXkyZMlST/5yU80Z84cvfPOO6qurtaaNWtUVVUlr9dbv7/H42m0LTIyUhcvXmxyHIIMAJzI5W7+IwgXLlzQ0aNH9cADD0iShg8frtjYWLlcLj366KMqKyuT1+uVz+erf00gEGi0zefzqV27dk2ORZABgBOZPLVYVFSkBx98UJJkGIaGDh2qiooKSdKuXbvUvXt39e7dW9u3b5cklZSUKCEhQV6vV6GhoTpx4oQMw1BhYaGSk5ObHIvFHgDgRCYvvz969Kg6dOjw3VAulxYsWKD09HS1adNGP/rRjzRq1Ch5PB7t2LFDqampMgxDCxculPTdApHp06errq5OKSkp6tWrV9OnYhiGYerZtFIR9zW9nBP/zV+cz/sVhHNF+S1dgmW0CZG+rW3pKqyhjUntRsQTS5r9Wv8Hv72BlVw/OjIAcCL+IBoAYGk2CjL7nAkAwJHoyADAibhpMADA0mw0tUiQAYAT0ZEBACyNjgwAYGl0ZAAAK3PZKMjs01sCAByJjgwAHMhOHRlBBgBOZJ8cI8gAwInoyAAAlkaQAQAsjSADAFianYKM5fcAAEujIwMAJ7JPQ0aQAYAT2WlqkSADAAciyAAAlkaQAQAsjSADAFibfXKM5fcAAGujIwMAB2JqEQBgaQQZAMDSCDIAgLXZJ8cIMgBwIjoyAICl2SnIWH4PALA0OjIAcCA7dWQEGQA4EEEGALA2++QYQQYATkRHBgCwNIIMAGBpdgoylt8DACyNjgwAnMg+DRlBBgBOZKepRYIMAByIIAvC4cOH9eqrr8rv96u6ulo//elPNWXKFO3Zs0dr1qzR66+/fsPGCgQCmjdvng4dOqSwsDAtWLBAcXFxN+z4AGA3ZgbZpk2btHnzZknSpUuXdPDgQa1cuVIvvfSSPB6PUlJSlJ6eftXP7pKSkkb7NsWUILtw4YIyMjKUl5en+Ph41dXV6bnnntOaNWvUuXPnGz7e1q1bVVNTo7Vr16qkpEQ5OTkqKCi44eMAgF2YGWTDhg3TsGHDJEnZ2dkaPny4srKylJeXp9jYWKWlpamsrEynTp264mf3lfZNTEy86nimBNm2bdvUr18/xcfHS5I8Ho8WLVqk0NBQFRcX1++3atUqbdmyRX6/X7feeqvy8/P11VdfKTMzUyEhIQoEAlq8eLHCw8M1depUGYahS5cuKTs7W926das/zr59+9S/f39JUlJSkkpLS804LQCwj5sws7h//34dOXJEzz//vFasWKGOHTtKklJSUrRz5059/fXXjT67q6qqVFNT02jfmx5kZ86cUWxsbINtkZGRDZ4HAgGdP39eK1askNvt1vjx47V//36Vl5erZ8+emjFjhvbu3auLFy/q0KFDio6O1iuvvKIjR46ourq6wbGqqqrk9Xrrn3s8HtXW1iokhEuAANBSli5dqsmTJzf6jI6MjNTJkyev+Nl9tX2bYsonfUxMjMrKyhpsO3nypCoqKuqfu91uhYaGKiMjQ23btlVFRYVqa2s1YsQILVu2TBMmTFBUVJSmTZumAQMG6NixY5o0aZJCQkI0ceLEBsf2er3y+Xz1zwOBwDVDbO/6F9W9S8wNOFtn8Bfnt3QJsJk2/DuzRZm92OPChQs6evSoHnjgAVVVVTX4jPb5fGrXrp2+/fbbRp/df/95/v2+TTHlV2ngwIFaunSpxowZo44dO+ry5cvKycnRQw89pC5dukiSysvLtXXrVq1fv15+v1/Dhg2TYRjatm2b+vTpo/T0dP3pT3/S8uXLNXToUN1555166623VFxcrNzcXK1cubJ+vN69e+vTTz/VkCFDVFJSooSEhGvWmDxyoRmnbkv+4nxF3Nf0xVZI54oI+2C1CZG+rW3pKqzBrMA3O8iKior04IMPSvqu2QgNDdWJEycUGxurwsJCpaenq6KiotFn99X2bYopb5HX61VOTo5mz54twzDk8/k0cOBAjR07Vnv27JEkxcXFKSIiQqmpqZKk9u3b68yZM0pKStKsWbNUUFCgQCCgzMxMxcTEKCMjQ6tXr1Ztba0mT57cYLxBgwZpx44dSk1NlWEYWriQkAKAppi9+v7o0aPq0KFD/fPs7GxNnz5ddXV1SklJUa9evXTvvfde8bP7Svs2xWUYhmHq2bRSdBjBoyMLDh1Z8OjIgmdWR3bPjI+a/drDrz52Ayu5fsxSA4AD2ejvoQkyAHAiO93Zg7vfAwAsjY4MABzIRg0ZQQYATuR22yfJCDIAcCA6MgCApdlpsQdBBgAOZKMcI8gAwIns1JGx/B4AYGl0ZADgQHbqyAgyAHAgG+UYQQYATkRHBgCwNBvlGEEGAE5ERwYAsDQb5RjL7wEA1kZHBgAOxNQiAMDSbJRjBBkAOBEdGQDA0myUYwQZADgRHRkAwNJslGMsvwcAWBsdGQA4EFOLAABLs1GOEWQA4ER0ZAAASyPIAACWZqMcI8gAwIns1JGx/B4AYGl0ZADgQDZqyAgyAHAiO00tEmQA4EA2yjGCDACcyG2jJCPIAMCBbJRjBBkAOJGdrpGx/B4AYGl0ZADgQG77NGQEGQA4kZ2mFgkyAHAgG+UYQQYATuSSfZKMIAMABzLzGtnSpUv1ySef6PLlyxozZoy6d++uf/mXf1F8fLwkacyYMRoyZIjy8/P12WefKSQkRC+++KJ69uyp48eP64UXXpDL5dI999yjrKwsud1Nr0skyADAgcy6RrZ7924VFxdr9erV8vv9euuttyRJv/71r/XMM8/U73fgwAHt2bNH69ev19/+9jdNmTJFGzdu1Msvv6ypU6eqX79+mjt3rrZt26ZBgwY1OSbL7wEAN0xhYaESEhI0efJk/eY3v9HDDz+s0tJSffbZZ3r66af14osvqqqqSvv27VNKSopcLpdiYmJUV1enb775RgcOHFDfvn0lSQMGDNDOnTuvOSYdGQA4kFmLPc6dO6fTp0/rD3/4g06dOqWJEycqLS1NI0eOVI8ePVRQUKDf//73ioqKUnR0dP3rIiMjdfHiRRmGUd8tfr/tWujIAMCB3C5Xsx9NiY6OVkpKisLCwtS5c2eFh4fr4YcfVo8ePSRJgwYNUllZmbxer3w+X/3rfD6foqKiGlwP8/l8ateu3bXPpZnvAQDAwlyu5j+a0qdPH33++ecyDEOVlZXy+/1KS0vTl19+KUnatWuXunfvrt69e6uwsFCBQECnT59WIBDQbbfdpsTERO3evVuStH37diUnJ1/zXJhaBAAHMmuxx8CBA1VUVKQRI0bIMAzNnTtXt912m+bPn6/Q0FDdcccdmj9/vrxer5KTkzV69GgFAgHNnTtXkjRr1izNmTNHubm56ty5swYPHnztczEMwzDlbFq5iPvSW7oEy/AX5/N+BeFcUX5Ll2AZbUKkb2tbugpraGNSuzFyxf9p9mvX/6r3Dazk+jX5Fp0+fbrJF8fExNzQYgAAN4djvo/sF7/4hVwuly5duqSzZ88qNjZWbrdbJ06cUGxsrD7++OObVScAAFfUZJB98sknkqRp06bp6aefrr/o9uWXX2r58uXmVwcAMIV9+rEgF3v813/9V4OVIz179tTRo0dNKwoAYC7H3f3+hz/8of71X/9VQ4YMUSAQ0Pvvv19/zywAgPXY6fvIgvo7sldffVUXLlxQRkaGpk+frtraWr388stm1wYAMInL5Wr2o7UJqiO75ZZb9Pzzz+vEiRNKSEjQt99+q7Zt25pdGwDAJK0wj5otqI5s165devLJJzVp0iSdPXtWjzzyiAoLC82uDQBgEjt1ZEEFWW5urt599121a9dO7du316pVq/TKK6+YXRsAANcU1NRiIBBQ+/bt65936dLFtIIAAOaz02KPoFctfvrpp3K5XLpw4YLeeecd7uoBABbWGqcImyuoqcXf/e53+o//+A/97W9/06BBg3Tw4EHNnz/f7NoAACZxXcejtQmqIysvL1dubm6DbVu2bNE//MM/mFIUAMBcjrnX4p///GfV1NRoyZIl+u1vf1u/vba2VkuXLiXIAMCibJRjTQdZVVWViouL5fP56r/oTJI8Ho+mTZtmenEAAHPY6RpZk0E2atQojRo1Srt27VJCQoJuv/12+f1+nTlzRnFxcTerRgAAriqoxR6HDx/WhAkTJEnffPONfvOb32jt2rWmFgYAMI/L1fxHaxNUkK1bt07vvPOOJOnuu+/Wpk2btGrVKlMLAwCYx+1yNfvR2gS1avHy5csKCwurfx4aGmpaQQAA87XCPGq2oILsZz/7mX75y1/q8ccfl/Td0vtHH33U1MIAAOax02IPl2EYRjA7fvTRRyoqKlJISIjuv/9+/exnPzO7NlN9duhsS5dgGQ93vZ33KwgTlu1p6RIs48hrj6vL9A9bugxLOPLa46Ycd8rmg81+bd5T3W5gJdcvqI5Mktq3b68uXbpo2LBh+vLLL82sCQBgMjt1ZEEt9nj77bf1xhtvaMWKFfL7/Zo7d67efPNNs2sDAOCaggqyzZs3680331RERISio6O1YcMGbdy40ezaAAAmcbua/2htgppadLvdDVYthoeHy+PxmFYUAMBcrTGQmiuoIOvbt68WLVokv9+vrVu3au3atXrggQfMrg0AYBLHXSObOXOm4uLi1LVrV7333nv66U9/qlmzZpldGwDAJI6cWuzVq5eqq6sVEhKiBx98UCEhQS94BAC0MjZqyILryN58800999xz+vrrr3Xq1ClNnDiRxR4AYGGOu0XVunXrtGnTJnm9XknS5MmTNWbMGA0fPtzU4gAAuJagguyWW25pMJXYtm1bRUZGmlYUAMBcQU3HWURQQRYbG6vRo0friSeeUEhIiP7yl7/I6/UqPz9fkpSenm5qkQCAG6sVzhA2W1BB1qlTJ3Xq1Ek1NTWqqanRT37yE7PrAgCYqDVe62quoO9+/+Mf/7jBto8++kiPPfaYKUUBAMxloxwLbpp00qRJWr58uSTp/Pnzmjp1qpYuXWpqYQAA89jp78iCCrJNmzapvLxcqampGjlypHr16qUNGzaYXRsAwCR2Wn4fVJAZhqHQ0FD5/X4ZhiGXyyW3205rXgAAVhVUGv385z/X3XffrY0bN2rdunUqKSnRyJEjza4NAGASl6v5j9amySB79913JUnLli3T4MGDFRISottuu01vvPGG/H7/TSkQAHDjOeYa2fr16yVJiYmJmjlzZoOf/c+vdQEAWIvrOv7X2jS5/N4wjCv+NwDA2lpjZ9VcQd/C3k7fXQMATmdmkC1dulSffPKJLl++rDFjxqhv37564YUX5HK5dM899ygrK0tut1v5+fn67LPPFBISohdffFE9e/bU8ePHr7hvk+fS1A8JLwCwJ5fL1exHU3bv3q3i4mKtXr1aK1euVEVFhV5++WVNnTpV7777rgzD0LZt23TgwAHt2bNH69evV25urrKzsyXpivteS5Md2eHDh/Xoo49KkiorK+v/2zAMff3110G9WQAA5ygsLFRCQoImT56sqqoqzZw5U+vWrVPfvn0lSQMGDNCOHTvUqVMnpaSkyOVyKSYmRnV1dfrmm2904MCBRvsOGjSoyTGbDLKPP/74Bp0aAKA1MWtq8dy5czp9+rT+8Ic/1H9/5fd/fyxJkZGRunjxoqqqqhQdHV3/uu+3X2nfa2kyyO6+++7rOB0AQGtl1pWj6Ohode7cWWFhYercubPCw8NVUVFR/3Ofz6d27drJ6/XK5/M12B4VFdXgetj3+14Lt+cAAAcy6xZVffr00eeffy7DMFRZWSm/368HH3xQu3fvliRt375dycnJ6t27twoLCxUIBHT69GkFAgHddtttSkxMbLTvtQS9ahEAYB9mTS0OHDhQRUVFGjFihAzD0Ny5c9WhQwfNmTNHubm56ty5swYPHiyPx6Pk5GSNHj1agUBAc+fOlSTNmjWr0b7XQpABgAOZuSj972+gIUmrVq1qtG3KlCmaMmVKg22dOnW64r5NYWoRAGBpdGQA4EDuVnirqeYiyADAgex0vwuCDAAcyJH3WgQA2Edr/Kbn5iLIAMCBbJRjBBkAOJGdOjKW3wMALI2ODAAcyEYNGUEGAE5kp+k4ggwAHMhOX5xMkAGAA9knxggyAHAkO61aJMgAwIHsE2P2ut4HAHAgOjIAcCAbzSwSZADgRKxaBABYmp2uKxFkAOBAdGQAAEuzT4wRZADgSHbqyOw0TQoAcCA6MgBwIDt1MQQZADiQnaYWCTIAcCD7xBhBBgCOZKOGjCADACdy26gnI8gAwIHs1JHZaeEKAMCB6MgAwIFcTC0CAKzMTlOLBBkAOBCLPQAAlkZHBgCwNIIMAGBpdlrswfJ7AICl0ZEBgAO57dOQEWQA4ER2mlokyADAgVjsAQCwNDoyAIClcY0MAGBpZndkTz31lLxerySpQ4cOeuSRR7Ro0SLdddddkqQpU6YoOTlZ8+bN06FDhxQWFqYFCxYoLi5OJSUleumll+TxeJSSkqL09PQmxyLIAAA31KVLl2QYhlauXFm/7fXXX9eMGTM0ePDg+m1btmxRTU2N1q5dq5KSEuXk5KigoEBZWVnKy8tTbGys0tLSVFZWpsTExKuOZ9rfkR0+fFhpaWkaN26chg8friVLlsgwDO3evVvTpk0zZcwvvvhC48aNM+XYAGAnLlfzH9dSXl4uv9+vZ555Rv/8z/+skpISHThwQBs3btTYsWOVk5Oj2tpa7du3T/3795ckJSUlqbS0VFVVVaqpqVHHjh3lcrmUkpKinTt3NjmeKR3ZhQsXlJGRoby8PMXHx6uurk7PPfec1qxZo86dO5sxpJYtW6b3339fERERphwfAOzEzInFNm3aaPz48Ro5cqSOHTumZ599VqNHj9Zjjz2mDh06KCsrS2vWrFFVVVX99KMkeTyeRtsiIyN18uTJJsczJci2bdumfv36KT4+vr64RYsWKTQ0VMXFxfX7rVq1Slu2bJHf79ett96q/Px8ffXVV8rMzFRISIgCgYAWL16s8PBwTZ06VYZh6NKlS8rOzla3bt0ajNmxY0fl5eVp5syZZpwSANiK28T19506dVJcXJxcLpc6deqk6Oho/fznP6+/Pvboo4/q448/VlRUlHw+X/3rAoGAvF5vg20+n0/t2rVrcjxTguzMmTOKjY1tsC0yMrLB80AgoPPnz2vFihVyu90aP3689u/fr/LycvXs2VMzZszQ3r17dfHiRR06dEjR0dF65ZVXdOTIEVVXVzcac/DgwTp16lTQNd4ff4siw7lEGKyHu97e0iW0ekdee7ylS7AU3q+WZWZHtmHDBv31r3/VvHnzVFlZqYsXL2rkyJHasGGDfvjDH2rXrl3q3r277rjjDn366acaMmSISkpKlJCQIK/Xq9DQUJ04cUKxsbEqLCxsmcUeMTExKisra7Dt5MmTqqioqH/udrsVGhqqjIwMtW3bVhUVFaqtrdWIESO0bNkyTZgwQVFRUZo2bZoGDBigY8eOadKkSQoJCdHEiROvu8aiY//vuo/hFA93vV2fHTrb0mW0ehOW7WnpEizjyGuPq8v0D1u6DEswLfBNTLIRI0YoMzNTY8aMkcvl0ssvv6zq6mqlp6erTZs2+tGPfqRRo0bJ4/Fox44dSk1NlWEYWrhwoSQpOztb06dPV11dnVJSUtSrV68mxzMlyAYOHKilS5dqzJgx6tixoy5fvqycnBw99NBD6tKli6TvLgZu3bpV69evl9/v17Bhw2QYhrZt26Y+ffooPT1df/rTn7R8+XINHTpUd955p9566y0VFxcrNze3wWoYAMD/jpnL78PCwrR48eJG21NSUhpt+93vftdoW1JSktatWxf0eKYEmdfrVU5OjmbPni3DMOTz+TRw4ECNHTtWe/Z896/WuLg4RUREKDU1VZLUvn17nTlzRklJSZo1a5YKCgoUCASUmZmpmJgYZWRkaPXq1aqtrdXkyZPNKBsAYEEuwzCMli6iJTBVFjymFoPD1GLwmFoMnllTi3v+b/Mvr/TtfMsNrOT6sdoBABzIRneoIsgAwJFslGQEGQA4EHe/BwBYGt9HBgCwNBvlmHk3DQYA4GagIwMAJ7JRS0aQAYADsdgDAGBpLPYAAFiajXKMIAMAR7JRkhFkAOBAdrpGxvJ7AICl0ZEBgAOx2AMAYGk2yjGCDAAcyUZJRpABgAPZabEHQQYADsQ1MgCApdkox1h+DwCwNjoyAHAiG7VkBBkAOBCLPQAAlsZiDwCApdkoxwgyAHAkGyUZQQYADmSna2QsvwcAWBodGQA4EIs9AACWZqMcI8gAwJFslGQEGQA4kJ0WexBkAOBAXCMDAFiajXKM5fcAAGujIwMAJ7JRS0aQAYADsdgDAGBpLPYAAFiajXKMIAMAJ6IjAwBYnH2SjCADANxwTz31lLxerySpQ4cOGj16tF566SV5PB6lpKQoPT1dgUBA8+bN06FDhxQWFqYFCxYoLi5OJSUljfZtCkEGAA5k5tTipUuXZBiGVq5cWb/tySefVF5enmJjY5WWlqaysjKdOnVKNTU1Wrt2rUpKSpSTk6OCggJlZWU12jcxMfGq4xFkAOBAZk4slpeXy+/365lnnlFtba2mTJmimpoadezYUZKUkpKinTt36uuvv1b//v0lSUlJSSotLVVVVdUV9yXIAAANmNmRtWnTRuPHj9fIkSN17NgxPfvss2rXrl39zyMjI3Xy5ElVVVXVTz9KksfjabTt+32bQpABgAOZ+QfRnTp1UlxcnFwulzp16qSoqCidP3++/uc+n0/t2rXTt99+K5/PV789EAjI6/U22Pb9vk3hXosA4ESu63hcw4YNG5STkyNJqqyslN/vV9u2bXXixAkZhqHCwkIlJyerd+/e2r59uySppKRECQkJ8nq9Cg0NbbRvU+jIAMCBzLxGNmLECGVmZmrMmDFyuVxauHCh3G63pk+frrq6OqWkpKhXr1669957tWPHDqWmpsowDC1cuFCSlJ2d3WjfJs/FMAzDxPNptT47dLalS7CMh7vezvsVhAnL9rR0CZZx5LXH1WX6hy1dhiUcee1xU45beeFys1/7g3ahN7CS60dHBgAOxJ09AACWxt3vAQDWZp8cI8gAwIlslGMEGQA4EdfIAACWZqdrZPxBNADA0ujIAMCB7DS1SEcGALA0OjIAcCA7dWQEGQA4kJ0WexBkAOBAdGQAAEuzUY6x2AMAYG10ZADgRDZqyQgyAHAgFnsAACyNxR4AAEuzUY4RZADgSDZKMoIMABzITtfIWH4PALA0OjIAcCA7LfZwGYZhtHQRAAA0F1OLAABLI8gAAJZGkAEALI0gg+OcOnVKPXr00JNPPql/+qd/0hNPPKFf//rXqqioaNbxNm3apBdeeEGS9Oyzz6qysvKq+y5ZskR79+79Xx2/a9euzaoLcAqCDI5055136r333tMf//hHffDBB+rRo4fmz59/3cddtmyZfvCDH1z150VFRaqrq7vucQD8N5bfA5KSk5P1ySef6JFHHlHPnj118OBBvfvuu/r888/19ttvKxAIqHv37srKylJ4eLj++Mc/qqCgQF6vV3fffbfatm0rSXrkkUf07//+72rfvr2ys7O1b98+hYaGatKkSaqpqVFpaalmz56t/Px8tWnTRvPmzdP58+fVpk0bzZkzR4mJiTp16pRmzJih6upq9erVq4XfGaD1oyOD412+fFkffvihevfuLUkaMGCAPv74Y33zzTdat26d1qxZo/fee0+333673nzzTVVWVuq1117TO++8o7Vr18rn8zU65sqVK1VdXa0PP/xQ//Zv/6bf//73GjJkiHr06KEFCxaoa9eumjVrlmbMmKHNmzdr/vz5mjZtmiRp/vz5GjZsmN577736mgBcHR0ZHOnMmTN68sknJUk1NTXq2bOnnn/+ee3YsaO+C9q9e7eOHz+uUaNGSfou8BITE1VcXKz77rtPd9xxhyTpH//xH/Wf//mfDY5fVFSkUaNGye12q3379vrggw8a/Nzn86m0tFSZmZn126qrq3Xu3Dnt2bNHixcvliQNHTpUs2fPNudNAGyCIIMjfX+N7ErCw8MlSXV1dXr88cfrg8Tn86murk67du1SIBCo3z8kpPH/jf5+2/Hjx3XXXXfVPw8EAgoLC2tQQ0VFhaKjoyVJ39+nwOVyyWWnWzAAJmBqEbiKfv366S9/+YvOnj0rwzA0b948vf322+rTp4+++OILVVZWKhAI6M9//nOj195///368MMPZRiGzp49q1/84heqqamRx+NRXV2doqKiFB8fXx9kO3bs0NNPPy1Jeuihh/T+++9LkrZs2aKampqbd9KABdGRAVfx4x//WOnp6frlL3+pQCCgbt26KS0tTeHh4Zo9e7Z+9atfKSIiQl26dGn02rFjx2rBggUaOnSoJGnOnDnyer3q37+/srKytGjRIr366quaN2+eli9frtDQUL3++utyuVyaO3euZsyYoTVr1ujee+9VZGTkzT51wFK41yIAwNKYWgQAWBpBBgCwNIIMAGBpBBkAwNIIMgCApRFkAABLI8gAAJZGkAEALO3/AxGtSSQNDfOxAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 576x396 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=models['XGBoost'].predict(X_test))\n",
    "print('Confusion matrix:\\n', conf_mat)\n",
    "\n",
    "labels = ['Class 0', 'Class 1']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Expected')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:\n",
      "[0.58216 0.57552 0.566   0.57984 0.57488]\n",
      "Mean accuracy: 0.58\n"
     ]
    }
   ],
   "source": [
    "stratified_shuffle_split = StratifiedShuffleSplit(test_size=.5, train_size=.5, n_splits=5)\n",
    "scores = cross_val_score(models['Random Forest'], X_train, y_train, cv=stratified_shuffle_split)\n",
    "print(\"Cross-validation scores:\\n{}\".format(scores))\n",
    "print(\"Mean accuracy: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[7573 4910]\n",
      " [5457 7060]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=models['Random Forest'].predict(X_test))\n",
    "print('Confusion matrix:\\n', conf_mat)\n",
    "\n",
    "labels = ['Class 0', 'Class 1']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Expected')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
